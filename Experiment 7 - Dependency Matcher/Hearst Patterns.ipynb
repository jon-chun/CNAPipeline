{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hearst Patterns\n",
    "---\n",
    "\n",
    "In this experiment we test Hearst Patterns which detect hypernym relations in a text. \n",
    "\n",
    "Hypernym relations are semantic relationships between two concepts: C1 is a hypernym of C2 means that C1 categorizes C2 (e.g. “instrument” is a hypernym of “Piano”). For this research, the phrase, \"America has enemies, such as Al Qaeda and the Taliban\" would return the following '[('Al Qaeda', 'enemy'), ('the Taliban', 'enemy')]'. In this example, the categorising term 'enemy' is a hypernym of both 'Al Qaeda' and the 'Taliban'; conversely 'al Qaeda' and 'the Tabliban' are hyponyms of 'enemy'. Using this technique, hypernym terms could be classified as ingroup or outgroup and named entities identified as hyponym terms could be identified as either group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment has not produced any results from the bin Laden text, but has produced some promising results from the Bush text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('al Qaeda', 'loosely affiliate terrorist organization')]\n",
      "[('al Qaeda', 'terrorist group')]\n",
      "[('Canada', 'close friend'), ('Australia', 'close friend'), ('Germany', 'close friend'), ('France', 'close friend'), ('force', 'the operation')]\n"
     ]
    }
   ],
   "source": [
    "true_positives = [\n",
    "    \"The evidence we have gathered all points to a collection of loosely affiliated terrorist organizations known as al Qaeda.\",\n",
    "    \"Terrorist groups like al Qaeda depend upon the aid or indifference of governments.\",\n",
    "    \"Other close friends, including Canada, Australia, Germany and France, have pledged forces as the operation unfolds.\",\n",
    "]\n",
    "             \n",
    "for sentence in true_positives:\n",
    "    print(h.find_hyponyms(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are some false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('e  mail', 'terrorist'), ('the internet', 'terrorist'), ('cell phone', 'terrorist')]\n",
      "[('the United States', 'a hostile regime')]\n"
     ]
    }
   ],
   "source": [
    "false_positives = [\n",
    "    \"This new law that I sign today will allow surveillance of all communications used by terrorists, including e-mails, the Internet, and cell phones.\",\n",
    "    \"From this day forward, any nation that continues to harbor or support terrorism will be regarded by the United States as a hostile regime.\"\n",
    "]\n",
    "\n",
    "for sentence in false_positives:\n",
    "    print(h.find_hyponyms(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "the following code is taken from: https://github.com/mmichelsonIF/hearst_patterns_python/blob/master/hearstPatterns/test/test_hearstPatterns.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "from spacy.pipeline import merge_noun_chunks\n",
    "from spacy.pipeline import merge_entities\n",
    "\n",
    "\n",
    "class HearstPatterns(object):\n",
    "\n",
    "    def __init__(self, extended=False, merge = False):\n",
    "\n",
    "        self.__adj_stopwords = [\n",
    "            'able', 'available', 'brief', 'certain',\n",
    "            'different', 'due', 'enough', 'especially', 'few', 'fifth',\n",
    "            'former', 'his', 'howbeit', 'immediate', 'important', 'inc',\n",
    "            'its', 'last', 'latter', 'least', 'less', 'likely', 'little',\n",
    "            'many', 'ml', 'more', 'most', 'much', 'my', 'necessary',\n",
    "            'new', 'next', 'non', 'old', 'other', 'our', 'ours', 'own',\n",
    "            'particular', 'past', 'possible', 'present', 'proud', 'recent',\n",
    "            'same', 'several', 'significant', 'similar', 'such', 'sup', 'sure'\n",
    "        ]\n",
    "\n",
    "        # now define the Hearst patterns\n",
    "        # format is <hearst-pattern>, <general-term>\n",
    "        # so, what this means is that if you apply the first pattern,\n",
    "        # the first Noun Phrase (NP)\n",
    "        # is the general one, and the rest are specific NPs\n",
    "        self.__hearst_patterns = [\n",
    "            (\n",
    "                '(NP_\\\\w+ (, )?such as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "            (\n",
    "                '(NP_\\\\w+ (, )?know as (NP_\\\\w+ ?(, )?(and |or )?)+)', # added for this experiment\n",
    "                'first'\n",
    "            ),\n",
    "            (\n",
    "                '(such NP_\\\\w+ (, )?as (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "            (\n",
    "                '((NP_\\\\w+ ?(, )?)+(and |or )?other NP_\\\\w+)',\n",
    "                'last'\n",
    "            ),\n",
    "            (\n",
    "                '(NP_\\\\w+ (, )?include (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "            (\n",
    "                '(NP_\\\\w+ (, )?especially (NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                'first'\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        if extended:\n",
    "            self.__hearst_patterns.extend([\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?any other NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?some other NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?be a NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?like (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    'such (NP_\\\\w+ (, )?as (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?like other NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?one of the NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?one of these NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?one of those NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    'example of (NP_\\\\w+ (, )?be (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?be example of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?for example (, )?'\n",
    "                    '(NP_\\\\w+ ?(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which be call NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which be name NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?mainly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?mostly (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?notably (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?particularly (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?principally (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?in particular (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?except (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?other than (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?e.g. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ \\\\( (e.g.|i.e.) (, )?(NP_\\\\w+ ? (, )?(and |or )?)+'\n",
    "                    '(\\\\. )?\\\\))',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?i.e. (, )?(NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? a kind of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? kind of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? form of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which look like NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?which sound like NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?which be similar to (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?example of this be (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?type (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )? NP_\\\\w+ type)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?whether (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(compare (NP_\\\\w+ ?(, )?)+(and |or )?with NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?compare to (NP_\\\\w+ ? (, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?among -PRON- (NP_\\\\w+ ? '\n",
    "                    '(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and |or )?as NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )? (NP_\\\\w+ ? (, )?(and |or )?)+ '\n",
    "                    'for instance)',\n",
    "                    'first'\n",
    "                ),\n",
    "                (\n",
    "                    '((NP_\\\\w+ ?(, )?)+(and|or)? sort of NP_\\\\w+)',\n",
    "                    'last'\n",
    "                ),\n",
    "                (\n",
    "                    '(NP_\\\\w+ (, )?which may include (NP_\\\\w+ '\n",
    "                    '?(, )?(and |or )?)+)',\n",
    "                    'first'\n",
    "                )\n",
    "            ])\n",
    "\n",
    "        self.__spacy_nlp = spacy.load('en_core_web_sm')\n",
    "        \n",
    "        if merge:\n",
    "            self.__spacy_nlp.add_pipe(nlp.create_pipe(\"merge_entities\"), after = \"ner\")\n",
    "            #self.__spacy_nlp.add_pipe(nlp.create_pipe(\"merge_noun_chunks\"), last = True)\n",
    "            \n",
    "\n",
    "    def chunk(self, rawtext):\n",
    "        doc = self.__spacy_nlp(rawtext)\n",
    "        chunks = []\n",
    "        for sentence in doc.sents:\n",
    "            sentence_text = sentence.lemma_\n",
    "            for chunk in sentence.noun_chunks:\n",
    "                if chunk.lemma_.lower() == \"example\":\n",
    "                    start = chunk.start\n",
    "                    pre_token = sentence[start - 1].lemma_.lower()\n",
    "                    post_token = sentence[start + 1].lemma_.lower()\n",
    "                    if start > 0 and\\\n",
    "                            (pre_token == \"for\" or post_token == \"of\"):\n",
    "                        continue\n",
    "                if chunk.lemma_.lower() == \"type\":\n",
    "                    continue\n",
    "                chunk_arr = []\n",
    "                replace_arr = []\n",
    "                # print(\"chunk:\", chunk)\n",
    "                for token in chunk:\n",
    "                    if token.lemma_ in self.__adj_stopwords + [\"i.e.\", \"e.g.\"]:\n",
    "                        continue\n",
    "                    chunk_arr.append(token.lemma_)\n",
    "                    # Remove punctuation and stopword adjectives\n",
    "                    # (generally quantifiers of plurals)\n",
    "                    if token.lemma_.isalnum():\n",
    "                        replace_arr.append(token.lemma_)\n",
    "                    else:\n",
    "                        replace_arr.append(''.join(\n",
    "                            char for char in token.lemma_ if char.isalnum()\n",
    "                        ))\n",
    "                if len(chunk_arr) == 0:\n",
    "                    chunk_arr.append(chunk[-1].lemma_)\n",
    "                chunk_lemma = ' '.join(chunk_arr)\n",
    "                # print(chunk_lemma)\n",
    "                replacement_value = 'NP_' + '_'.join(replace_arr)\n",
    "                if chunk_lemma:\n",
    "                    sentence_text = re.sub(r'\\b%s\\b' % re.escape(chunk_lemma),\n",
    "                                           r'%s' % replacement_value,\n",
    "                                           sentence_text)\n",
    "            chunks.append(sentence_text)\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "    \"\"\"\n",
    "        This is the main entry point for this code.\n",
    "        It takes as input the rawtext to process and returns a list\n",
    "        of tuples (specific-term, general-term)\n",
    "        where each tuple represents a hypernym pair.\n",
    "    \"\"\"\n",
    "    def find_hyponyms(self, rawtext):\n",
    "\n",
    "        hyponyms = []\n",
    "        np_tagged_sentences = self.chunk(rawtext)\n",
    "\n",
    "        for sentence in np_tagged_sentences:\n",
    "            # two or more NPs next to each other should be merged\n",
    "            # into a single NP, it's a chunk error\n",
    "\n",
    "            for (hearst_pattern, parser) in self.__hearst_patterns:\n",
    "                matches = re.search(hearst_pattern, sentence)\n",
    "                if matches:\n",
    "                    match_str = matches.group(0)\n",
    "\n",
    "                    nps = [a for a in match_str.split() if a.startswith(\"NP_\")]\n",
    "\n",
    "                    if parser == \"first\":\n",
    "                        general = nps[0]\n",
    "                        specifics = nps[1:]\n",
    "                    else:\n",
    "                        general = nps[-1]\n",
    "                        specifics = nps[:-1]\n",
    "\n",
    "                    for i in range(len(specifics)):\n",
    "                        pair = (\n",
    "                            self.clean_hyponym_term(specifics[i]),\n",
    "                            self.clean_hyponym_term(general)\n",
    "                        )\n",
    "                        # reduce duplicates\n",
    "                        if pair not in hyponyms:\n",
    "                            hyponyms.append(pair)\n",
    "\n",
    "        return hyponyms\n",
    "\n",
    "    def clean_hyponym_term(self, term):\n",
    "        # good point to do the stemming or lemmatization\n",
    "        return term.replace(\"NP_\", \"\").replace(\"_\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have seen it in the courage of passengers, who rushed terrorists to save others on the ground -- passengers like an exceptional man named Todd Beamer.\n",
      "[('an exceptional man', 'passenger')]\n",
      "==========\n",
      "The evidence we have gathered all points to a collection of loosely affiliated terrorist organizations known as al Qaeda.\n",
      "[('al Qaeda', 'loosely affiliate terrorist organization')]\n",
      "==========\n",
      "The terrorists' directive commands them to kill Christians and Jews, to kill all Americans, and make no distinctions among military and civilians, including women and children.\n",
      "[('woman', 'civilian'), ('child', 'civilian')]\n",
      "==========\n",
      "This group and its leader -- a person named Usama bin Laden -- are linked to many other organizations in different countries, including the Egyptian Islamic Jihad and the Islamic Movement of Uzbekistan.\n",
      "[('the egyptian Islamic Jihad', 'country'), ('the Islamic Movement', 'country')]\n",
      "==========\n",
      "They are recruited from their own nations and neighborhoods and brought to camps in places like Afghanistan, where they are trained in the tactics of terror.\n",
      "[('Afghanistan', 'place')]\n",
      "==========\n",
      "Release all foreign nationals, including American citizens, you have unjustly imprisoned.\n",
      "[('american citizen', 'all foreign national')]\n",
      "==========\n",
      "They want to overthrow existing governments in many Muslim countries, such as Egypt, Saudi Arabia, and Jordan.\n",
      "[('Egypt', 'muslim country'), ('Saudi Arabia', 'muslim country'), ('Jordan', 'muslim country')]\n",
      "==========\n",
      "By sacrificing human life to serve their radical visions -- by abandoning every value except the will to power -- they follow in the path of fascism, Nazism, and totalitarianism.\n",
      "[('the will', 'every value')]\n",
      "==========\n",
      "From this day forward, any nation that continues to harbor or support terrorism will be regarded by the United States as a hostile regime.\n",
      "\n",
      "[('the United States', 'a hostile regime')]\n",
      "==========\n",
      "But the only way to defeat terrorism as a threat to our way of life is to stop it, eliminate it, and destroy it where it grows.\n",
      "[('terrorism', 'a threat')]\n",
      "==========\n",
      "These carefully targeted actions are designed to disrupt the use of Afghanistan as a terrorist base of operations and to attack the military capability of the Taliban regime.\n",
      "\n",
      "[('Afghanistan', 'a terrorist base')]\n",
      "==========\n",
      "Other close friends, including Canada, Australia, Germany and France, have pledged forces as the operation unfolds.\n",
      "[('Canada', 'close friend'), ('Australia', 'close friend'), ('Germany', 'close friend'), ('France', 'close friend'), ('force', 'the operation')]\n",
      "==========\n",
      "More than two weeks ago, I gave Taliban leaders a series of clear and specific demands: Close terrorist training camps; hand over leaders of the Al Qaeda network; and return all foreign nationals, including American citizens, unjustly detained in your country.\n",
      "[('american citizen', 'all foreign national')]\n",
      "==========\n",
      "Initially the terrorists may burrow deeper into caves and other entrenched hiding places.\n",
      "[('cave', 'entrenched hiding place')]\n",
      "==========\n",
      "It is enough to know that evil, like goodness, exists.  \n",
      "[('goodness', 'evil')]\n",
      "==========\n",
      "This new law that I sign today will allow surveillance of all communications used by terrorists, including e-mails, the Internet, and cell phones.\n",
      "\n",
      "[('e  mail', 'terrorist'), ('the internet', 'terrorist'), ('cell phone', 'terrorist')]\n",
      "==========\n",
      "Those names include a citizen of Gambia, whose wife spent their fourth wedding anniversary, September the 12th, searching in vain for her husband.\n",
      "[('a citizen', 'those name')]\n",
      "==========\n",
      "Those names include a man who supported his wife in Mexico, sending home money every week.  \n",
      "[('a man', 'those name'), ('who', 'those name')]\n",
      "==========\n",
      "Those names include a young Pakistani who prayed toward Mecca five times a day, and died that day trying to save others.\n",
      "\n",
      "[('a young Pakistani', 'those name'), ('who', 'those name')]\n",
      "==========\n",
      "All of the victims, including Muslims, were killed with equal indifference and equal satisfaction by the terrorist leaders.  \n",
      "[('Muslims', 'the victim')]\n",
      "==========\n",
      "The terrorists are violating the tenets of every religion, including the one they invoke.\n",
      "\n",
      "[('the one', 'every religion')]\n",
      "==========\n",
      "Terrorist groups like al Qaeda depend upon the aid or indifference of governments.  \n",
      "[('al Qaeda', 'terrorist group')]\n",
      "==========\n",
      "And my country grieves for all the suffering the Taliban have brought upon Afghanistan, including the terrible burden of war.  \n",
      "[('the terrible burden', 'Afghanistan')]\n",
      "==========\n",
      "Even before this current crisis, 4 million Afghans depended on food from the United States and other nations, and millions of Afghans were refugees from Taliban oppression.\n",
      "\n",
      "[('the United States', 'nation')]\n",
      "==========\n",
      "Many nations, including mine, are sending food and medicine to help Afghans through the winter.  \n",
      "[('mine', 'nation')]\n",
      "==========\n",
      "My country is pledged to investing in education and combating AIDS and other infectious diseases around the world.  \n",
      "[('AIDS', 'infectious disease')]\n",
      "==========\n",
      "The brave men and women of our military are rewriting the rules of war with new technologies and old values like courage and honor.  \n",
      "[('courage', 'value'), ('honor', 'value')]\n",
      "==========\n",
      "Sophisticated systems like Global Hawk, an unmanned surveillance plane, are transforming our intelligence capabilities.  \n",
      "[('Global Hawk', 'sophisticated system'), ('an unmanned surveillance plane', 'sophisticated system')]\n",
      "==========\n",
      "We are privileged to have with us the families of many of the heroes on September the 11th, including the family of Jeremy Glick of Flight 93.\n",
      "[('the family', 'the 11th')]\n",
      "==========\n",
      "These enemies view the entire world as a battlefield, and we must pursue them wherever they are.\n",
      "[('the entire world', 'a battlefield')]\n",
      "==========\n",
      "A terrorist underworld -- including groups like Hamas, Hezbollah, Islamic Jihad, Jaish-i\n",
      "[('Hamas', 'group'), ('Hezbollah', 'group'), ('Islamic Jihad', 'group')]\n",
      "==========\n",
      "We will develop vaccines to fight anthrax and other deadly diseases.\n",
      "[('anthrax', 'deadly disease')]\n",
      "==========\n",
      "We need mentors to love children, especially children whose parents are in prison.  \n",
      "[('child', 'child'), ('whose parent', 'child')]\n",
      "==========\n",
      "America will take the side of brave men and women who advocate these values around the world, including the Islamic world, because we have a greater objective than eliminating threats and containing resentment.\n",
      "[('the islamic world', 'the world')]\n",
      "==========\n",
      "They embrace tyranny and death as a cause and a creed.\n",
      "[('death', 'a cause')]\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentences = []\n",
    "\n",
    "h = HearstPatterns(extended=True, merge = False)\n",
    "\n",
    "\n",
    "# filepath = \"C:/Users/Steve/OneDrive - University of Southampton/CulturalViolence/KnowledgeBases/Experiment 7 - Dependency Matcher/\"\n",
    "# filename = \"bush_ingroup_sents.json\"\n",
    "\n",
    "# with open(os.path.join(filepath, filename), 'r') as fp:\n",
    "#     sentences = json.load(fp)\n",
    "\n",
    "filepath = \"C:/Users/Steve/OneDrive - University of Southampton/CulturalViolence/KnowledgeBases/Speeches/George Bush/\"\n",
    "filename = \"bush_complete.txt\"\n",
    "\n",
    "# filepath = \"C:/Users/Steve/OneDrive - University of Southampton/CulturalViolence/KnowledgeBases/Speeches/Osama bin Laden/\"\n",
    "# filename = \"binladen_complete.txt\"\n",
    "\n",
    "file = os.path.join(filepath, filename)\n",
    "\n",
    "_, file_extension = os.path.splitext(file)\n",
    "\n",
    "if file_extension == \".json\":\n",
    "    with open(file, 'r') as fp:\n",
    "        sentences = json.load(fp)\n",
    "        \n",
    "elif file_extension == \".txt\":\n",
    "    with open(file, 'r') as text:\n",
    "        sentences = {key:value.text for (key,value) in enumerate(nlp(text.read()).sents)}\n",
    "        \n",
    "else:\n",
    "    raise SystemExit(\"file not found!\")\n",
    "    \n",
    "    \n",
    "found = False\n",
    "    \n",
    "for sent in sentences.values():\n",
    "    hyponyms = h.find_hyponyms(sent)\n",
    "    if hyponyms:\n",
    "        print(sent)\n",
    "        print(hyponyms)\n",
    "        print('==========')\n",
    "        found = True\n",
    "\n",
    "if found == False:\n",
    "    print('none found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: C:\\Users\\Steve\\AppData\\Roaming\\jupyter\\runtime\\kernel-a0e680ba-0f0e-4639-850d-cefc20bead94 (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "AttributeError: module '__main__' has no attribute 'C:\\Users\\Steve\\AppData\\Roaming\\jupyter\\runtime\\kernel-a0e680ba-0f0e-4639-850d-cefc20bead94'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "class TestHearstPatterns(unittest.TestCase):\n",
    "\n",
    "    def test_hyponym_finder(self):\n",
    "        h = HearstPatterns(extended=True)\n",
    "\n",
    "        # H1\n",
    "        hyps1 = h.find_hyponyms(\"Forty-four percent of patients with uveitis had one or more identifiable signs or symptoms, such as red eye, ocular pain, visual acuity, or photophobia, in order of decreasing frequency.\")\n",
    "\n",
    "        self.assertEqual(tuple(map(str.lower, hyps1[0])), (\"red eye\", \"symptom\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps1[1])), (\"ocular pain\", \"symptom\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps1[2])), (\"visual acuity\", \"symptom\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps1[3])), (\"photophobia\", \"symptom\"))\n",
    "\n",
    "        # H2\n",
    "        hyps2 = h.find_hyponyms(\"There are works by such authors as Herrick, Goldsmith, and Shakespeare.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps2[0])), (\"herrick\", \"author\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps2[1])), (\"goldsmith\", \"author\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps2[2])), (\"shakespeare\", \"author\"))\n",
    "\n",
    "        # H3\n",
    "        hyps3 = h.find_hyponyms(\"There were bruises, lacerations, or other injuries were not prevalent.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps3[0])), (\"bruise\", \"injury\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps3[1])), (\"laceration\", \"injury\"))\n",
    "\n",
    "        # H4\n",
    "        hyps4 = h.find_hyponyms(\"common law countries, including Canada, Australia, and England enjoy toast.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps4[0])), (\"canada\", \"common law country\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps4[1])), (\"australia\", \"common law country\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps4[2])), (\"england\", \"common law country\"))\n",
    "\n",
    "        # H5\n",
    "        hyps5 = h.find_hyponyms(\"Many countries, especially France, England and Spain also enjoy toast.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps5[0])), (\"france\", \"country\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps5[1])), (\"england\", \"country\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps5[2])), (\"spain\", \"country\"))\n",
    "\n",
    "        # H2\n",
    "        hyps6 = h.find_hyponyms(\"There are such benefits as postharvest losses reduction, food increase and soil fertility improvement.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps6[0])), (\"postharvest loss reduction\", \"benefit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps6[1])), (\"food increase\", \"benefit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps6[2])), (\"soil fertility improvement\", \"benefit\"))\n",
    "\n",
    "        # H'1\n",
    "        hyps7 = h.find_hyponyms(\"Fruits, i.e. , apples, bananas, oranges and peaches.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[0])), (\"apple\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[1])), (\"banana\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[2])), (\"orange\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[3])), (\"peach\", \"fruit\"))\n",
    "\n",
    "        hyps7 = h.find_hyponyms(\"Fruits, e.g. apples, bananas, oranges and peaches.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[0])), (\"apple\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[1])), (\"banana\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[2])), (\"orange\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps7[3])), (\"peach\", \"fruit\"))\n",
    "\n",
    "        # H'2\n",
    "\n",
    "        hyps10 = h.find_hyponyms(\"Fruits (e.g. apples, bananas, oranges and peaches.)\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[0])), (\"apple\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[1])), (\"banana\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[2])), (\"orange\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[3])), (\"peach\", \"fruit\"))\n",
    "\n",
    "        hyps10 = h.find_hyponyms(\"Fruits (i.e. apples, bananas, oranges and peaches.)\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[0])), (\"apple\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[1])), (\"banana\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[2])), (\"orange\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps10[3])), (\"peach\", \"fruit\"))\n",
    "\n",
    "        # H'3\n",
    "        hyps8 = h.find_hyponyms(\"Fruits, for example apples, bananas, oranges and peaches.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps8[0])), (\"apple\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps8[1])), (\"banana\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps8[2])), (\"orange\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps8[3])), (\"peach\", \"fruit\"))\n",
    "\n",
    "        # H'4\n",
    "        hyps9 = h.find_hyponyms(\"Fruits, which may include apples, bananas, oranges and peaches.\")\n",
    "        self.assertEqual(tuple(map(str.lower, hyps9[0])), (\"apple\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps9[1])), (\"banana\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps9[2])), (\"orange\", \"fruit\"))\n",
    "        self.assertEqual(tuple(map(str.lower, hyps9[3])), (\"peach\", \"fruit\"))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hearstPatterns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9ba955f80f0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munittest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhearstPatterns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhearstPatterns\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHearstPatterns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mTestHearstPatterns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munittest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTestCase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hearstPatterns'"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "from hearstPatterns.hearstPatterns import HearstPatterns\n",
    "\n",
    "class TestHearstPatterns(unittest.TestCase):\n",
    "\n",
    "    def test_hyponym_finder(self):\n",
    "        h = HearstPatterns()\n",
    "        hyps1 =  h.find_hyponyms(\"Forty-four percent of patients with uveitis had one or more identifiable signs or symptoms, such as red eye, ocular pain, visual acuity, or photophobia, in order of decreasing frequency.\")\n",
    "\n",
    "        self.assertEqual(hyps1[0], (\"red eye\", \"symptom\"))\n",
    "        self.assertEqual(hyps1[1], (\"ocular pain\", \"symptom\"))\n",
    "        self.assertEqual(hyps1[2], (\"visual acuity\", \"symptom\"))\n",
    "        self.assertEqual(hyps1[3], (\"photophobia\", \"symptom\"))\n",
    "\n",
    "        hyps2 = h.find_hyponyms(\"There are works by such authors as Herrick, Goldsmith, and Shakespeare.\")\n",
    "        self.assertEqual(hyps2[0], (\"herrick\", \"author\"))\n",
    "        self.assertEqual(hyps2[1], (\"goldsmith\", \"author\"))\n",
    "        self.assertEqual(hyps2[2], (\"shakespeare\", \"author\"))\n",
    "\n",
    "        hyps3 = h.find_hyponyms(\"There were bruises, lacerations, or other injuries were not prevalent.\")\n",
    "        self.assertEqual(hyps3[0], (\"bruise\", \"injury\"))\n",
    "        self.assertEqual(hyps3[1], (\"laceration\", \"injury\"))\n",
    "\n",
    "        hyps4 =  h.find_hyponyms(\"common law countries, including Canada, Australia, and England enjoy toast.\")\n",
    "        self.assertEqual(hyps4[0], (\"canada\", \"common law country\"))\n",
    "        self.assertEqual(hyps4[1], (\"australia\", \"common law country\"))\n",
    "        self.assertEqual(hyps4[2], (\"england\", \"common law country\"))\n",
    "\n",
    "        hyps5 = h.find_hyponyms(\"Many countries, especially France, England and Spain also enjoy toast.\")\n",
    "        self.assertEqual(hyps5[0], (\"france\", \"country\"))\n",
    "        self.assertEqual(hyps5[1], (\"england\", \"country\"))\n",
    "        self.assertEqual(hyps5[2], (\"spain\", \"country\"))\n",
    "\n",
    "        hyps6 = h.find_hyponyms(\"There are such benefits as postharvest losses reduction, food increase and soil fertility improvement.\")\n",
    "        self.assertEqual(hyps6[0], (\"postharvest loss reduction\", \"benefit\"))\n",
    "        self.assertEqual(hyps6[1], (\"food increase\", \"benefit\"))\n",
    "        self.assertEqual(hyps6[2], (\"soil fertility improvement\", \"benefit\"))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
