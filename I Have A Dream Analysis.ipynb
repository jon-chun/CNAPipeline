{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "On February 24th, 1920, the first great mass meeting under the auspices of the new movement took place. In the Banquet Hall of the Hofbräuhaus in Munich the twenty-five theses which constituted the programme of our new party were expounded to an audience of nearly two thousand people and each thesis was enthusiastically received. \n",
      "\n",
      "Thus we brought to the knowledge of the public those first principles and lines of action along which the new struggle was to be conducted for the abolition of a confused mass of obsolete ideas and opinions which had obscure and often pernicious tendencies. A new force was to make its appearance among the timid and feckless bourgeoisie. This force was destined to impede the triumphant advance of the Marxists and bring the Chariot of Fate to a standstill just as it seemed about to reach its goal. \n",
      "\n",
      "It was evident that this new movement could gain the public significance and support which are necessary pre-requisites in such a gigantic struggle only if it succeeded from the very outset in awakening a sacrosanct conviction in the hearts of its followers, that here it was not a case of introducing a new electoral slogan into the political field but that an entirely new world view, which was of a radical significance, had to be promoted. \n",
      "\n",
      "One must try to recall the miserable jumble of opinions that used to be arrayed side by side to form the usual Party Programme, as it was called, and one must remember how these opinions used to be brushed up or dressed in a new form from time to time. If we would properly understand these programmatic monstrosities we must carefully investigate the motives which inspired the average bourgeois 'programme committee'. \n",
      "\n",
      "Those people are always influenced by one and the same preoccupation when they introduce something new into their programme or modify something already contained in it. That preoccupation is directed towards the results of the next election. The moment these artists in parliamentary government have the first glimmering of a suspicion that their darling public may be ready to kick up its heels and escape from the harness of the old party wagon they begin to paint the shafts with new colours. On such occasions the party astrologists and horoscope readers, the so-called 'experienced men' and 'experts', come forward. For the most part they are old parliamentary hands whose political schooling has furnished them with ample experience. They can remember former occasions when the masses showed signs of losing patience and they now diagnose the menace of a similar situation arising. Resorting to their old prescription, they form a 'committee'. They go around among the darling public and listen to what is being said. They dip their noses into the newspapers and gradually begin to scent what it is that their darlings, the broad masses, are wishing for, what they reject and what they are hoping for. The groups that belong to each trade or business, and even office employees, are carefully studied and their innermost desires are investigated. The 'malicious slogans' of the opposition from which danger is threatened are now suddenly looked upon as worthy of reconsideration, and it often happens that these slogans, to the great astonishment of those who originally coined and circulated them, now appear to be quite harmless and indeed are to be found among the dogmas of the old parties. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### firstly open the file\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "filepath  = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CulturalViolence\\KnowledgeBases\\data\"\n",
    "\n",
    "with open(os.path.join(filepath, \"IHaveADream.txt\"), 'r') as text:\n",
    "    speech_text = text.readlines()\n",
    "\n",
    "for line in speech_text[:5]:\n",
    "    print(line)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with a score of  1.0 , the most positive paragraph is\n",
      "A state which in this age of racial poisoning dedicates itself to the care of its best racial elements must some day become lord of the earth.\n",
      "\n",
      "\n",
      "with a score of  -1.0 , the most negative paragraph is\n",
      "Times of national collapse are determined by the preponderating influence of the worst elements. \n",
      "\n",
      "Wall time: 942 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## now we get the most positive and negative sentiment of the text\n",
    "from textblob import TextBlob\n",
    "\n",
    "counter = 0\n",
    "most_pos_para = ''\n",
    "most_neg_para = ''\n",
    "most_pos_score = 0\n",
    "most_neg_score = 0\n",
    "\n",
    "\n",
    "for section in speech_text:\n",
    "    paragraph = TextBlob(section)\n",
    "    \n",
    "    if paragraph.sentiment.polarity > most_pos_score:\n",
    "        most_pos_para = paragraph\n",
    "        most_pos_score = paragraph.sentiment.polarity\n",
    "    elif paragraph.sentiment.polarity < most_neg_score:\n",
    "        most_neg_para = paragraph\n",
    "        most_neg_score = paragraph.sentiment.polarity\n",
    "        \n",
    "print('with a score of ', str(most_pos_score), ', the most positive paragraph is')\n",
    "print(most_pos_para)\n",
    "print()\n",
    "print('with a score of ', str(most_neg_score), ', the most negative paragraph is')\n",
    "print(most_neg_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "['february', '24th', '1920', 'first', 'great', 'mass', 'meeting', 'auspex', 'new', 'movement', 'took', 'place', 'banquet', 'hall', 'hofbräuhaus', 'munich', 'twentyfive', 'thesis', 'constituted', 'programme', 'new', 'party', 'expounded', 'audience', 'nearly', 'two', 'thousand', 'people', 'thesis', 'enthusiastically', 'received']\n",
      "[('new', 2), ('thesis', 2), ('february', 1), ('24th', 1), ('1920', 1)]\n",
      "[('new', 2), ('thesis', 2), ('february', 1)]\n",
      "-----\n",
      "['thus', 'brought', 'knowledge', 'public', 'first', 'principle', 'line', 'action', 'along', 'new', 'struggle', 'conducted', 'abolition', 'confused', 'mass', 'obsolete', 'idea', 'opinion', 'obscure', 'often', 'pernicious', 'tendency', 'new', 'force', 'make', 'appearance', 'among', 'timid', 'feckless', 'bourgeoisie', 'force', 'destined', 'impede', 'triumphant', 'advance', 'marxist', 'bring', 'chariot', 'fate', 'standstill', 'seemed', 'reach', 'goal']\n",
      "[('new', 2), ('force', 2), ('thus', 1), ('brought', 1), ('knowledge', 1)]\n",
      "[('new', 2), ('force', 2), ('thus', 1)]\n",
      "-----\n",
      "['evident', 'new', 'movement', 'could', 'gain', 'public', 'significance', 'support', 'necessary', 'prerequisite', 'gigantic', 'struggle', 'succeeded', 'outset', 'awakening', 'sacrosanct', 'conviction', 'heart', 'follower', 'case', 'introducing', 'new', 'electoral', 'slogan', 'political', 'field', 'entirely', 'new', 'world', 'view', 'radical', 'significance', 'promoted']\n",
      "[('new', 3), ('significance', 2), ('evident', 1), ('movement', 1), ('could', 1)]\n",
      "[('new', 3), ('significance', 2), ('evident', 1)]\n",
      "-----\n",
      "['one', 'must', 'try', 'recall', 'miserable', 'jumble', 'opinion', 'used', 'arrayed', 'side', 'side', 'form', 'usual', 'party', 'programme', 'called', 'one', 'must', 'remember', 'opinion', 'used', 'brushed', 'dressed', 'new', 'form', 'time', 'time', 'would', 'properly', 'understand', 'programmatic', 'monstrosity', 'must', 'carefully', 'investigate', 'motif', 'inspired', 'average', 'bourgeois', 'programme', 'committee']\n",
      "[('must', 3), ('one', 2), ('opinion', 2), ('used', 2), ('side', 2)]\n",
      "[('must', 3), ('one', 2), ('opinion', 2)]\n",
      "-----\n",
      "['people', 'always', 'influenced', 'one', 'preoccupation', 'introduce', 'something', 'new', 'programme', 'modify', 'something', 'already', 'contained', 'it', 'preoccupation', 'directed', 'towards', 'result', 'next', 'election', 'moment', 'artist', 'parliamentary', 'government', 'first', 'glimmering', 'suspicion', 'darling', 'public', 'may', 'ready', 'kick', 'heel', 'escape', 'harness', 'old', 'party', 'wagon', 'begin', 'paint', 'shaft', 'new', 'colour', 'occasion', 'party', 'astrologist', 'horoscope', 'reader', 'socalled', 'experienced', 'men', 'expert', 'come', 'forward', 'part', 'old', 'parliamentary', 'hand', 'whose', 'political', 'schooling', 'furnished', 'ample', 'experience', 'remember', 'former', 'occasion', 'mass', 'showed', 'sign', 'losing', 'patience', 'diagnose', 'menace', 'similar', 'situation', 'arising', 'resorting', 'old', 'prescription', 'form', 'committee', 'go', 'around', 'among', 'darling', 'public', 'listen', 'said', 'dip', 'nose', 'newspaper', 'gradually', 'begin', 'scent', 'darling', 'broad', 'mass', 'wishing', 'for', 'reject', 'hoping', 'for', 'group', 'belong', 'trade', 'business', 'even', 'office', 'employee', 'carefully', 'studied', 'innermost', 'desire', 'investigated', 'malicious', 'slogan', 'opposition', 'danger', 'threatened', 'suddenly', 'looked', 'upon', 'worthy', 'reconsideration', 'often', 'happens', 'slogan', 'great', 'astonishment', 'originally', 'coined', 'circulated', 'them', 'appear', 'quite', 'harmless', 'indeed', 'found', 'among', 'dogma', 'old', 'party']\n",
      "[('old', 4), ('darling', 3), ('party', 3), ('preoccupation', 2), ('something', 2)]\n",
      "[('old', 4), ('darling', 3), ('party', 3)]\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "## now we clean the document \n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from collections import Counter\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop]) ## remove stop words\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude) ## remove puncutation\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split()) ## lemmatize each word\n",
    "    return normalized \n",
    "    \n",
    "\n",
    "def Most_Common(lst):\n",
    "    data = Counter(lst)\n",
    "    return data.most_common(5)\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in speech_text]\n",
    "print('done')\n",
    "\n",
    "### print the most common words from the first 5 lines.\n",
    "\n",
    "for line in doc_clean[:5]:\n",
    "    print(line)\n",
    "    print(Most_Common(line)) \n",
    "    print(Counter(line).most_common(3))\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.009*\"meeting\" + 0.008*\"one\" + 0.007*\"germany\" + 0.006*\"people\" + 0.005*\"would\"'), (1, '0.015*\"must\" + 0.012*\"state\" + 0.009*\"people\" + 0.007*\"national\" + 0.007*\"would\"'), (2, '0.010*\"would\" + 0.009*\"state\" + 0.008*\"people\" + 0.006*\"one\" + 0.006*\"german\"')]\n"
     ]
    }
   ],
   "source": [
    "# Creating the term dictionary for the dataset, where every unique term is assigned an index. \n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "print(ldamodel.print_topics(num_topics=5, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Text Topics: [(0, '0.009*\"german\" + 0.009*\"people\"'), (1, '0.009*\"meeting\" + 0.007*\"one\"'), (2, '0.013*\"state\" + 0.010*\"would\"')]\n",
      "-----\n",
      "Text Overall Sentiment: 0.0964078892045762\n",
      "-----\n",
      "Text Common Nouns: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### analyse the whole document to extract the topics and sentiment\n",
    "# https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "# https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import gensim\n",
    "import collections\n",
    "from gensim import corpora\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "### open file\n",
    "filepath  = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CulturalViolence\\KnowledgeBases\\data\"\n",
    "\n",
    "with open(os.path.join(filepath, \"MeinKampfv2.txt\"), 'r') as text:\n",
    "    speech_text = text.readlines()\n",
    "\n",
    "# define common functions\n",
    "def clean(doc): # text cleaning function\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop]) # remove stopwords\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude) # remove punctuation\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split()) #lemmatize words\n",
    "    return normalized\n",
    "\n",
    "def Most_Common(lst, quantity):\n",
    "    data = Counter(lst)\n",
    "    return data.most_common(quantity)\n",
    "\n",
    "# speech pre-processing to a bag-of-words\n",
    "i = TextBlob(''.join(speech_text))\n",
    "doc_clean = [clean(doc).split() for doc in speech_text] # clean speech to a bag of words\n",
    "dictionary = corpora.Dictionary(doc_clean) # Creating the term dictionary for the speech, where every unique term is assigned an index.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean] # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "print('Overall Text Topics: ', end='')\n",
    "print(ldamodel.print_topics(num_topics=5, num_words=2))\n",
    "print('-----')\n",
    "\n",
    "print('Text Overall Sentiment: ', end='')\n",
    "print(i.sentiment.polarity)\n",
    "print('-----')\n",
    "\n",
    "print('Text Common Nouns: ', end='')\n",
    "print(Most_Common(i.noun_phrases, 15))\n",
    "print('-----')\n",
    "\n",
    "flat_list = []\n",
    "\n",
    "for sublist in doc_clean:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "\n",
    "\n",
    "print('Speech common words: ', end='')\n",
    "print(Most_Common(flat_list, 5))\n",
    "print('-----')\n",
    "\n",
    "# generate speech summary data\n",
    "\n",
    "counter = 0\n",
    "most_pos_para = ''\n",
    "most_neg_para = ''\n",
    "pos_paras = []\n",
    "neg_paras = []\n",
    "most_pos_score = 0\n",
    "most_neg_score = 0\n",
    "\n",
    "for section in speech_text:\n",
    "    paragraph = TextBlob(section)\n",
    "    \n",
    "    if paragraph.sentiment.polarity > most_pos_score and paragraph.sentiment.polarity < 1:\n",
    "        most_pos_para = paragraph\n",
    "        most_pos_score = paragraph.sentiment.polarity\n",
    "    elif paragraph.sentiment.polarity < most_neg_score and paragraph.sentiment.polarity > -1:\n",
    "        most_neg_para = paragraph\n",
    "        most_neg_score = paragraph.sentiment.polarity\n",
    "    elif paragraph.sentiment.polarity == 1:\n",
    "        pos_paras.append(section)\n",
    "    elif paragraph.sentiment.polarity == -1:\n",
    "        neg_paras.append(section)\n",
    "    \n",
    "        \n",
    "print('With a score of ', str(most_pos_score), ', the most positive paragraph is:')\n",
    "print(most_pos_para)\n",
    "print('-----')\n",
    "print('With a score of ', str(most_neg_score), ', the most negative paragraph is:')\n",
    "print(most_neg_para)\n",
    "print('-----')\n",
    "print('The paragraphs with a sentiment score of 1 are:')\n",
    "for i in pos_paras:\n",
    "    print(i)\n",
    "print()\n",
    "print('The paragraphs with a sentiment score of -1 are:')\n",
    "for i in neg_paras:\n",
    "    print(i)\n",
    "print('-----')\n",
    "\n",
    "# generate paragraph summary data by iterating through each paragraph\n",
    "\n",
    "for section in speech_text:\n",
    "    counter += 1\n",
    "    paragraph = TextBlob(section)\n",
    "    \n",
    "    doc_clean = [clean(section).split()] # clean section\n",
    "    dictionary = corpora.Dictionary(doc_clean) # Creating the term dictionary of each paragraph, where every unique term is assigned an index. \n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean] # Converting paragraph into Document Term Matrix using dictionary prepared above.\n",
    "    if doc_term_matrix: ## skip over empty paragraphs\n",
    "        ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "    \n",
    "    print('Paragraph %d' %counter)\n",
    "    print('    Paragraph topics: ', end='')\n",
    "    print(ldamodel.print_topics(num_topics=5, num_words=2))\n",
    "    print('    Paragraph sentiment: ', end='')\n",
    "    print(paragraph.sentiment.polarity)\n",
    "    print('    Paragraph nouns: ', end='')\n",
    "    print(paragraph.noun_phrases)\n",
    "    print('    Paragraph common words: ', end='')\n",
    "    \n",
    "    for w in doc_clean:\n",
    "        print(Most_Common(w, 5))\n",
    "        \n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
