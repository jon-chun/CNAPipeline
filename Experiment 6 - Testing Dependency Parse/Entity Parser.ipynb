{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening document\n",
      "17358\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "# open file\n",
    "print('Opening document')\n",
    "\n",
    "with open('C:/Users/Steve/Documents/Cultural Violence/GBSpeech.txt', 'r') as text:\n",
    "#text = open('C:/Users/Steve/Documents/Cultural Violence/we-they-parsed-GBSpeech.txt', 'r')\n",
    "    doc = text.read()\n",
    "\n",
    "print(len(doc))\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initialise libraries\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.pipeline import merge_entities\n",
    "from spacy.pipeline import merge_noun_chunks\n",
    "\n",
    "print('setting up pipeline')\n",
    "spacy_nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "print('applying pipelines')\n",
    "spacy_nlp.add_pipe(merge_entities, after = 'ner')\n",
    "spacy_nlp.add_pipe(merge_noun_chunks, after='ner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading doc\n",
      "108600\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "## OBL Speeches\n",
    "\n",
    "print('loading doc')\n",
    "\n",
    "FileList = ['19960823-OBL Declaration.txt',\n",
    "            '20011007-OBL Full Warning.txt',\n",
    "            '20011109-OBL.txt',\n",
    "            '20021124-OBL Letter to America.txt',\n",
    "            '20041101-Al Jazeera Speech.txt'\n",
    "           ]\n",
    "doc = ''\n",
    "\n",
    "for f in FileList:\n",
    "    with open('C:/Users/Steve/Documents/Cultural Violence/Osama bin Laden/' + f, 'r') as text:\n",
    "        doc = doc + text.read()\n",
    "\n",
    "print(len(doc))\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying pipelines\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E007] 'merge_entities' already exists in pipeline. Existing names: ['tagger', 'parser', 'ner', 'merge_noun_chunks', 'merge_entities']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-dbd7b21765dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'applying pipelines'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mspacy_nlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerge_entities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mafter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ner'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mspacy_nlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerge_noun_chunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ner'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[1;34m(self, component, name, before, after, first, last)\u001b[0m\n\u001b[0;32m    263\u001b[0m                 \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE007\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbefore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mafter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE006\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E007] 'merge_entities' already exists in pipeline. Existing names: ['tagger', 'parser', 'ner', 'merge_noun_chunks', 'merge_entities']"
     ]
    }
   ],
   "source": [
    "# Applying pipeline\n",
    "\n",
    "new_doc = spacy_nlp(doc)\n",
    "\n",
    "print('complete', 'doc length = ', len(new_doc)) # merged document contains 2981 entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Speaker, Mr. President Pro Tempore, members of Congress, and fellow Americans:\n",
      "Congress                                 ORG                  PROPN | NOUN | ADP\n",
      "-----\n",
      "As a symbol of America's resolve, my administration will work with Congress, and these two leaders, to show the world that we will rebuild New York City.\n",
      "Congress                                 ORG                  VERB | VERB | ADP\n",
      "-----\n",
      "This group and its leader -- a person named Osama bin Laden -- are linked to many other organizations in different countries, including the Egyptian Islamic Jihad and the Islamic Movement of Uzbekistan.\n",
      "the Egyptian Islamic Jihad               ORG                  ADP | NOUN | VERB\n",
      "-----\n",
      "This group and its leader -- a person named Osama bin Laden -- are linked to many other organizations in different countries, including the Egyptian Islamic Jihad and the Islamic Movement of Uzbekistan.\n",
      "the Islamic Movement of Uzbekistan       ORG                  NOUN | VERB | PROPN\n",
      "-----\n",
      "The thousands of FBI agents who are now at work in this investigation may need your cooperation, and I ask you to give it.\n",
      "FBI agents                               ORG                  VERB | NOUN | ADP\n",
      "-----\n",
      "Al Qaeda is to terror what the mafia is to crime.\n",
      "Al Qaeda                                 ORG                  VERB | VERB | VERB\n",
      "-----\n",
      "The leadership of al Qaeda has great influence in Afghanistan and supports the Taliban regime in controlling most of that country.\n",
      "al Qaeda                                 ORG                  VERB | NOUN | ADP\n",
      "-----\n",
      "Our war on terror begins with al Qaeda, but it does not end there.\n",
      "al Qaeda                                 ORG                  VERB | VERB | ADP\n",
      "-----\n",
      "The evidence we have gathered all points to a collection of loosely affiliated terrorist organizations known as al Qaeda.\n",
      "al Qaeda                                 ORG                  NOUN | VERB | ADP\n",
      "-----\n",
      "So tonight I announce the creation of a Cabinet-level position reporting directly to me -- the Office of Homeland Security.\n",
      "the Office of Homeland Security          ORG                  VERB | VERB | NOUN\n",
      "-----\n",
      "I've called the Armed Forces to alert, and there is a reason.\n",
      "the Armed Forces                         ORG                  VERB | VERB | VERB\n",
      "-----\n",
      "Deliver to United States authorities all the leaders of al Qaeda who hide in your land.\n",
      "al Qaeda                                 ORG                  NOUN | NOUN | ADP\n",
      "-----\n",
      "The terrorists are traitors to their own faith, trying, in effect, to hijack Islam itself.\n",
      "Islam                                    ORG                  VERB | VERB | VERB\n",
      "-----\n",
      "In Afghanistan, we see al Qaeda's vision for the world.\n",
      "al Qaeda's vision                        ORG                  VERB | VERB | VERB\n",
      "-----\n",
      "Many will be involved in this effort, from FBI agents to intelligence operatives to the reservists we have called to active duty.\n",
      "FBI agents                               ORG                  VERB | VERB | ADP\n",
      "-----\n",
      "The terrorists practice a fringe form of Islamic extremism that has been rejected by Muslim scholars and the vast majority of Muslim clerics -- a fringe movement that perverts the peaceful teachings of Islam.\n",
      "Islam                                    ORG                  VERB | NOUN | ADP\n",
      "-----\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "#### this cell is looking at how to parse the the named entities and noun chunks from the document\n",
    "\n",
    "ncset = set()\n",
    "for ent in new_doc.noun_chunks:\n",
    "    ncset.add(ent)\n",
    "\n",
    "entset = set()\n",
    "\n",
    "for ent in new_doc.ents:\n",
    "    if ent.label_ in 'ORG': # {'GPE', 'ORG', 'NORP', 'PERSON', 'FAC'}:\n",
    "        entset.add(ent)\n",
    "\n",
    "for ent in entset:\n",
    "    print(ent.sent.text.strip())\n",
    "    dep = ent.root.head\n",
    "    n = 40\n",
    "    print(f\"{str(ent).ljust(n)} {str(ent.label_).ljust(20)} {dep.head.head.pos_} | {dep.head.pos_} | {dep.pos_}\")\n",
    "    print('-----')\n",
    "\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entset = set()\n",
    "entarray = list(new_doc.noun_chunks)\n",
    "print(type(entarray))\n",
    "\n",
    "entset = set(entarray)\n",
    "print(len(entarray))\n",
    "# 222 before\n",
    "\n",
    "for ent in entset:\n",
    "    n = 40\n",
    "    print(str(ent).ljust(n), str(ent.label_).ljust(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean document, note: stop words removes pronouns which are vitial to understand the otherising dynamics\n",
    "\n",
    "print('cleaning document')\n",
    "doc_array = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print('Tokenizing document')\n",
    "\n",
    "for sentence in new_doc.sents:\n",
    "    doc_array.append([token for token in sentence if \n",
    "                      (not token.is_stop and not token.is_punct and token.text.find('\\n') and not token.is_space and token.text !=\"'s\")])\n",
    "    \n",
    "for sentence in doc_array:\n",
    "    for i, token in enumerate(sentence):\n",
    "        if token.lemma_ != '-PRON-':\n",
    "            print(token, ' | ', token.lemma_)\n",
    "            sentence[i] = token.lemma_\n",
    "\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they : 24\n",
      "Token Head           Token Pos            Sentence Object      Token Ancestors\n",
      "-----                -----                -----                -----               \n",
      "They want to overthrow existing governments in many Muslim countries, such as Egypt, Saudi Arabia, and Jordan.\n",
      "want                 nsubj                [existing governments, many Muslim countries, Egypt]['want']             \n",
      "\n",
      "We will come together to strengthen our intelligence capabilities to know the plans of terrorists before they act, and find them before they strike.\n",
      "act                  nsubj                [our intelligence capabilities, the plans, terrorists, them]['act', 'know', 'strengthen', 'come']\n",
      "\n",
      "They are recruited from their own nations and neighborhoods and brought to camps in places like Afghanistan, where they are trained in the tactics of terror.\n",
      "trained              nsubjpass            [their own nations, camps, places, Afghanistan, the tactics, terror]['trained', 'Afghanistan', 'like', 'places', 'in', 'brought', 'recruited']\n",
      "\n",
      "They hate our freedoms -- our freedom of religion, our freedom of speech, our freedom to vote and assemble and disagree with each other.\n",
      "hate                 nsubj                [our freedoms, religion, speech, other]['hate']             \n",
      "\n",
      "By sacrificing human life to serve their radical visions -- by abandoning every value except the will to power -- they follow in the path of fascism, and Nazism, and totalitarianism.\n",
      "follow               nsubj                [human life, their radical visions, every value, the will, power, the path, fascism]['follow']           \n",
      "\n",
      "They hate what we see right here in this chamber -- a democratically elected government.\n",
      "hate                 nsubj                [what, this chamber, government]['hate']             \n",
      "\n",
      "Give the United States full access to terrorist training camps, so we can make sure they are no longer operating.\n",
      "operating            nsubj                [full access, terrorist training camps]['operating', 'sure', 'make']\n",
      "\n",
      "They are sent back to their homes or sent to hide in countries around the world to plot evil and destruction.\n",
      "sent                 nsubjpass            [their homes, countries, the world, evil]['sent']             \n",
      "\n",
      "They are the heirs of all the murderous ideologies of the 20th century.\n",
      "are                  nsubj                [all the murderous ideologies, the 20th century]['are']              \n",
      "\n",
      "These were the true strengths of our economy before September 11th, and they are our strengths today.\n",
      "are                  nsubj                [our economy, September 11th]['are', 'were']      \n",
      "\n",
      "They will hand over the terrorists, or they will share in their fate.\n",
      "share                nsubj                [the terrorists, their fate]['share', 'hand']    \n",
      "\n",
      "They stand against us, because we stand in their way.\n",
      "stand                nsubj                [us, their way]      ['stand']            \n",
      "\n",
      "They will hand over the terrorists, or they will share in their fate.\n",
      "hand                 nsubj                [the terrorists, their fate]['hand']             \n",
      "\n",
      "They did not touch its source.\n",
      "touch                nsubj                [its source]         ['touch']            \n",
      "\n",
      "They are the same murderers indicted for bombing American embassies in Tanzania and Kenya, and responsible for bombing the USS Cole.\n",
      "are                  nsubj                [American embassies, Tanzania, the USS Cole]['are']              \n",
      "\n",
      "With every atrocity, they hope that America grows fearful, retreating from the world and forsaking our friends.\n",
      "hope                 nsubj                [every atrocity, the world, our friends]['hope']             \n",
      "\n",
      "They are recruited from their own nations and neighborhoods and brought to camps in places like Afghanistan, where they are trained in the tactics of terror.\n",
      "recruited            nsubjpass            [their own nations, camps, places, Afghanistan, the tactics, terror]['recruited']        \n",
      "\n",
      "They understand that if this terror goes unpunished, their own cities, their own citizens may be next.\n",
      "understand           nsubj                [their own cities]   ['understand']       \n",
      "\n",
      "Americans are asking, why do they hate us?\n",
      "hate                 nsubj                [us]                 ['hate', 'asking']   \n",
      "\n",
      "And they will follow that path all the way, to where it ends: in history's unmarked grave of discarded lies.\n",
      "follow               nsubj                [that path, history's unmarked grave, discarded lies]['follow']           \n",
      "\n",
      "Americans have known wars -- but for the past 136 years, they have been wars on foreign soil, except for one Sunday in 1941.\n",
      "been                 nsubj                [wars, the past 136 years, foreign soil, one Sunday, 1941]['been', 'known']    \n",
      "\n",
      "They want to drive Christians and Jews out of vast regions of Asia and Africa.\n",
      "want                 nsubj                [Christians, vast regions, Asia]['want']             \n",
      "\n",
      "They want to drive Israel out of the Middle East.\n",
      "want                 nsubj                [Israel, the Middle East]['want']             \n",
      "\n",
      "We will come together to strengthen our intelligence capabilities to know the plans of terrorists before they act, and find them before they strike.\n",
      "strike               nsubj                [our intelligence capabilities, the plans, terrorists, them]['strike', 'find', 'come']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### what I'm trying to do here is parse the sentence an identifying word is part of to understand the relationships\n",
    "\n",
    "ingroup_statement = []\n",
    "outgroup_statement = []\n",
    "\n",
    "ingroup_posessions = []\n",
    "outgroup_posessions = []\n",
    "\n",
    "ingroup_descriptions = []\n",
    "outgroup_descriptions = []\n",
    "\n",
    "ingroup_prons = {'we'}\n",
    "outgroup_prons = {'they'}\n",
    "\n",
    "ingroup_posession = {'our'}\n",
    "outgroup_posession = {'their'}\n",
    "\n",
    "ingroup_nouns = {}\n",
    "outgroup_nouns = {'terrorist', 'terrorists'}\n",
    "\n",
    "biblical_set = {'god', 'evil'}\n",
    "ideology_set = {'nazism'}\n",
    "crime_set = {'murder', 'murderers', 'murderous', 'terrorist', 'terrorism'}\n",
    "\n",
    "def doc_categoriser(doc, words): # create an array of parsed statements based on identifying words\n",
    "    array = []\n",
    "    for token in doc:\n",
    "        if token.lower_ in words:\n",
    "            #if token.head.dep_ != 'ROOT':\n",
    "            array.append([str(token.sent).strip(),\n",
    "                          token.head,\n",
    "                          token.dep_,\n",
    "                          [obj for obj in token.sent if obj.dep_ in {'obj', 'pobj', 'dobj'}],\n",
    "                          [ancestor.text for ancestor in token.ancestors]])\n",
    "    return array\n",
    "\n",
    "def statement_display(doc, words, array):\n",
    " \n",
    "    array = doc_categoriser(doc, words)\n",
    "    \n",
    "    thisset = set()\n",
    "    for span in array: #create a span of the identifying words\n",
    "        thisset.add(a for a in span)\n",
    "    \n",
    "    print(*words, ': ' + str(len(array)))\n",
    "    \n",
    "    n = 20 # create table headings\n",
    "    print('Token Head'.ljust(n), 'Token Pos'.ljust(n), 'Sentence Object'.ljust(n), 'Token Ancestors')\n",
    "    print('-----'.ljust(n), '-----'.ljust(n), '-----'.ljust(n), '-----'.ljust(n))        \n",
    "\n",
    "\n",
    "    for s in thisset:\n",
    "        n = 21\n",
    "        for i, token in enumerate(s):\n",
    "            #for token in span:\n",
    "            if i==0:\n",
    "                print(token)\n",
    "            elif i>0:\n",
    "                #print(str(*words).ljust(n), end='')\n",
    "                print(str(token).ljust(n), end='')\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "    return\n",
    "            \n",
    "#statement_display(new_doc, ingroup_prons, ingroup_statement)\n",
    "#statement_display(new_doc, ingroup_posession, ingroup_posessions)\n",
    "statement_display(new_doc, outgroup_prons, outgroup_statement)\n",
    "#statement_display(new_doc, outgroup_posession, outgroup_posessions)\n",
    "\n",
    "#statement_display(new_doc, outgroup_nouns, ingroup_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
