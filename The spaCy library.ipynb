{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Cultural Violence using Natural Language processing.\n",
    "--\n",
    "\n",
    "We seek to detect cultural violence in natural language through measuring the self-other gradient. Cultural violence is a theory proposed by Johan Galtung which seeks to explain how aspects of culture  - religion, ideology, language, art along with formal and empirical science - are used to legitimise violence. This notebook is focusses on the representation of religion and ideology in language. Galtung explores how each aspect can be used as a mode of influence to create a ‘self-other gradient’ between ‘Chosen People’ - referred to as an ingroup - and others deemed ‘lower down the scale of worthiness’ - referred to as an outgroup. A general thesis follows whereby the steeper the gradient, the more legitimate violence becomes. To measure the self-other gradient, therefore, means establishing a schema for measuring how aspects of culture are used to elevate the self as an ingroup and debase the other as an outgroup.\n",
    "\n",
    "## Contributions\n",
    "The main contributions of this paper are as follows\n",
    "1. Propose cultural violence as a guiding theory for detecting harmful content in natural language.\n",
    "2. Propose a novel methodology for detecting harmful content in natural language.\n",
    "3. Identifying where current general purpose NLP technologies fall short in the specific task of detecting cultural violence\n",
    "4. A new nlp pipeline workflow for annotating the ingroup andoutgroups in natural language texts.\n",
    "\n",
    "## Methodology\n",
    "In having proposed cultural violence as a guiding theory, we proposed the following three step methodology.\n",
    "1. Detect the ingroup and outgroup of an orator's text\n",
    "2. Detect how each mode of influence is used for elevating the ingroup and debase the outgroup to create a gradient between each\n",
    "3. Devise a schema for measuring the gradient\n",
    "\n",
    "For the detectiong of cultural violence in natural langugage we test existing Natural Language Processing (NLP) technologies for each step of the methodology from which new pipeline components have been devised. NLP is a branch of artificial intelligence which seeks to process natural language to derive meaning. In general terms there are two fields of NLP, theoretical and applied. Theoretical NLP is concerned with the technical aspects of processing language and Applied NLP is concerned with the application of such technologies. The technologies we test are from the spaCy library for Python. This research sits within applied NLP and in seeking to qualify sociological theory sits within the field of the Digital Humanities.\n",
    "\n",
    "## Dataset\n",
    "The datasets we are using for these test are speech transcripts of George Bush and Osama bin Laden from the War on Terror in which they advocated violence, therefore, these datasets contain cultural violence. As a reference dataset we use speeches made by Martin Luther King who uses many of the same aspects of culture used by Bush and bin Laden, but he does not advocate for violence. The difference between Luther King's and the other speeches, therefore, may reveal the defining features of culturally violent language.\n",
    "\n",
    "## Experiments\n",
    "The tests are as follows:\n",
    "1. A test of named entity recognition in the spaCy language models against the dataset.\n",
    "2. A test of sentiment analysis technologies for detecting the ingroup and outgroup of a text.\n",
    "3. Detailed testing of the Watson API\n",
    "\n",
    "From these tests, the following spaCy pipeline components have been created:\n",
    "1. Supplemtary Named Entity Recognition - contains corrections for named entities either not identified or incorrectly identified by the model.\n",
    "2. A typology of cultural violence based on Mike Martin's 'Why We Fight' and Social Identity Theory for detecting aspects of culture. \n",
    "3. Named Concept Recognition - Using the cultural violence typology, and seed words from each speech, a component for detecting concepts related to each aspect of culture.\n",
    "\n",
    "This notebook now begins with Identifying the ingroup and outgroup of a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The spaCy pipeline\n",
    "\n",
    "The spacy pipeline, shown in the image above is based on a series of components shown in the table below.\n",
    "\n",
    "The processing pipeline always depends on the statistical model and its capabilities. For example, a pipeline can only include an entity recognizer component if the model includes data to make predictions of entity labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"200\" viewBox=\"0 0 923 200\" width=\"923\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "    <style>\n",
       "        .svg__pipeline__text { fill: #1a1e23; font: 20px Arial, sans-serif }\n",
       "        .svg__pipeline__text-small { fill: #1a1e23; font: bold 18px Arial, sans-serif }\n",
       "        .svg__pipeline__text-code { fill: #1a1e23; font: 600 16px Menlo, Monaco, Consolas, &quot;Liberation Mono&quot;, &quot;Courier New&quot;, monospace }\n",
       "    </style>\n",
       "    <rect fill=\"none\" height=\"127\" rx=\"19.1\" ry=\"19.1\" stroke=\"#09a3d5\" stroke-dasharray=\"3 6\" stroke-width=\"3\" width=\"601\" x=\"159\" y=\"21\"/>\n",
       "    <path d=\"M801 55h120v60H801z\" fill=\"#e1d5e7\" stroke=\"#9673a6\" stroke-width=\"2\"/>\n",
       "    <text class=\"svg__pipeline__text\" dy=\"0.75em\" height=\"19\" transform=\"translate(846.5 75.5)\" width=\"28\">Doc</text>\n",
       "    <path d=\"M121.2 84.7h29.4\" fill=\"none\" stroke=\"#999\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <path d=\"M156.6 84.7l-8 4 2-4-2-4z\" fill=\"#999\" stroke=\"#999\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <path d=\"M1 55h120v60H1z\" fill=\"#f5f5f5\" stroke=\"#999\" stroke-width=\"2\"/>\n",
       "    <text class=\"svg__pipeline__text\" dy=\"0.85em\" height=\"22\" transform=\"translate(43.5 73.5)\" width=\"34\">Text</text>\n",
       "    <path d=\"M760 84.7h33\" fill=\"none\" stroke=\"#999\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <path d=\"M799 84.7l-8 4 2-4-2-4z\" fill=\"#999\" stroke=\"#999\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <rect fill=\"#dae8fc\" height=\"39\" rx=\"5.8\" ry=\"5.8\" stroke=\"#09a3d5\" stroke-width=\"2\" width=\"75\" x=\"422\" y=\"1\"/>\n",
       "    <text class=\"svg__pipeline__text-code\" dx=\"0.1em\" dy=\"0.8em\" height=\"17\" transform=\"translate(444.5 11.5)\" width=\"29\">nlp</text>\n",
       "    <path d=\"M176 58h103.3L296 88l-16.8 30H176l16.8-30z\" fill=\"#f8cecc\" stroke=\"#b85450\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <text class=\"svg__pipeline__text-small\" dx=\"-0.25em\" dy=\"0.75em\" height=\"14\" transform=\"translate(206.5 80.5)\" width=\"58\">tokenizer</text>\n",
       "    <path d=\"M314 58h103.3L434 88l-16.8 30H314l16.8-30z\" fill=\"#ffe6cc\" stroke=\"#d79b00\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <text class=\"svg__pipeline__text-small\" dx=\"8\" dy=\"0.75em\" height=\"14\" transform=\"translate(342.5 80.5)\" width=\"62\">tagger</text>\n",
       "    <path d=\"M296.5 88.2h24.7\" fill=\"none\" stroke=\"#999\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <path d=\"M327.2 88.2l-8 4 2-4-2-4z\" fill=\"#999\" stroke=\"#999\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <path d=\"M416 58h103.3L536 88l-16.8 30H416l16.8-30z\" fill=\"#ffe6cc\" stroke=\"#d79b00\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <text class=\"svg__pipeline__text-small\" dx=\"-0.25em\" dy=\"0.75em\" height=\"14\" transform=\"translate(455.5 80.5)\" width=\"40\">parser</text>\n",
       "    <path d=\"M519 58h103.3L639 88l-16.8 30H519l16.8-30z\" fill=\"#ffe6cc\" stroke=\"#d79b00\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <text class=\"svg__pipeline__text-small\" dx=\"8\" dy=\"0.75em\" height=\"14\" transform=\"translate(558.5 80.5)\" width=\"40\">ner</text>\n",
       "    <path d=\"M622 58h103.3L742 88l-16.8 30H622l16.8-30z\" fill=\"#ffe6cc\" stroke=\"#d79b00\" stroke-miterlimit=\"10\" stroke-width=\"2\"/>\n",
       "    <text class=\"svg__pipeline__text-small\" dx=\"8\" dy=\"0.75em\" height=\"14\" transform=\"translate(671.5 80.5)\" width=\"20\">...</text>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "{\n",
    "    \"tags\": [\n",
    "        \"hide_input\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "from IPython.display import SVG\n",
    "image_weblink = 'https://spacy.io/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg'\n",
    "display(SVG(url= image_weblink))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Components\n",
    "\n",
    "A table describing the function of each component is shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component</th>\n",
       "      <th>Creates Objects</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tokenizer</th>\n",
       "      <td>Tokenizer</td>\n",
       "      <td>Doc</td>\n",
       "      <td>Segment text into tokens.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tagger</th>\n",
       "      <td>Tagger</td>\n",
       "      <td>Doc[i].tag</td>\n",
       "      <td>Assign part-of-speech tags.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parser</th>\n",
       "      <td>Dependency Parser</td>\n",
       "      <td>Doc[i].head, Doc[i].dep, Doc.sents, Doc.noun_chunks</td>\n",
       "      <td>Assign dependency labels.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ner</th>\n",
       "      <td>Entity Recognizer</td>\n",
       "      <td>Doc.ents, Doc[i].ent_iob, Doc[i].ent_type</td>\n",
       "      <td>Detect and label named entities.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textcat</th>\n",
       "      <td>Text Categorizer</td>\n",
       "      <td>Doc.cats</td>\n",
       "      <td>Assign document labels.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>Custom Components</td>\n",
       "      <td>Doc._.xxx, Token._.xxx, Span._.xxx</td>\n",
       "      <td>Assign custom attributes, methods or properties.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Component  \\\n",
       "Tokenizer  Tokenizer           \n",
       "Tagger     Tagger              \n",
       "Parser     Dependency Parser   \n",
       "ner        Entity Recognizer   \n",
       "textcat    Text Categorizer    \n",
       "...        Custom Components   \n",
       "\n",
       "                                               Creates Objects  \\\n",
       "Tokenizer  Doc                                                   \n",
       "Tagger     Doc[i].tag                                            \n",
       "Parser     Doc[i].head, Doc[i].dep, Doc.sents, Doc.noun_chunks   \n",
       "ner        Doc.ents, Doc[i].ent_iob, Doc[i].ent_type             \n",
       "textcat    Doc.cats                                              \n",
       "...        Doc._.xxx, Token._.xxx, Span._.xxx                    \n",
       "\n",
       "                                                Description  \n",
       "Tokenizer  Segment text into tokens.                         \n",
       "Tagger     Assign part-of-speech tags.                       \n",
       "Parser     Assign dependency labels.                         \n",
       "ner        Detect and label named entities.                  \n",
       "textcat    Assign document labels.                           \n",
       "...        Assign custom attributes, methods or properties.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option(\"display.max_columns\", 999)\n",
    "pd.set_option(\"display.max_rows\", 999)\n",
    "\n",
    "{\n",
    "    \"tags\": [\n",
    "        \"hide_input\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "Col1 = \"Component\"\n",
    "Col2 = \"Creates Objects\"\n",
    "Col3  = \"Description\"\n",
    "\n",
    "pipeline = {\n",
    "    \n",
    "    \"Tokenizer\" : {Col1 : \"Tokenizer\", Col2 : \"Doc\", Col3 : \"Segment text into tokens.\"},\n",
    "    \"Tagger\" : {Col1 : \"Tagger\", Col2 : \"Doc[i].tag\", Col3 : \"Assign part-of-speech tags.\"},\n",
    "    \"Parser\" : {Col1 : \"Dependency Parser\", Col2 : \"Doc[i].head, Doc[i].dep, Doc.sents, Doc.noun_chunks\", Col3 : \"Assign dependency labels.\"},\n",
    "    \"ner\" : {Col1 : \"Entity Recognizer\", Col2 : \"Doc.ents, Doc[i].ent_iob, Doc[i].ent_type\",Col3 : \"Detect and label named entities.\"},\n",
    "    \"textcat\" : {Col1 : \"Text Categorizer\", Col2 : \"Doc.cats\", Col3 : \"Assign document labels.\"},\n",
    "    \"...\" : {Col1 : \"Custom Components\", Col2 : \"Doc._.xxx, Token._.xxx, Span._.xxx\", Col3 : \"Assign custom attributes, methods or properties.\"}\n",
    "                    }  \n",
    "\n",
    "display(pd.DataFrame(pipeline).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Components\n",
    "\n",
    "Based on the experiments conducted to develop the methodology, we add the following custom build pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component</th>\n",
       "      <th>Creates Objects</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ner</th>\n",
       "      <td>Additional Named Entity Recognition</td>\n",
       "      <td>Doc.ents, Doc[i].ent_iob, Doc[i].ent_type</td>\n",
       "      <td>Specific tuning for the dataset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ncr</th>\n",
       "      <td>Named Concept Recognition</td>\n",
       "      <td>Doc._.named_concepts, span._.concept, span._.attribute, span._.feature, span._.classification</td>\n",
       "      <td>Named Concepts from the group schema.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Component  \\\n",
       "ner  Additional Named Entity Recognition   \n",
       "ncr  Named Concept Recognition             \n",
       "\n",
       "                                                                                   Creates Objects  \\\n",
       "ner  Doc.ents, Doc[i].ent_iob, Doc[i].ent_type                                                       \n",
       "ncr  Doc._.named_concepts, span._.concept, span._.attribute, span._.feature, span._.classification   \n",
       "\n",
       "                               Description  \n",
       "ner  Specific tuning for the dataset        \n",
       "ncr  Named Concepts from the group schema.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Col1 = \"Component\"\n",
    "Col2 = \"Creates Objects\"\n",
    "Col3  = \"Description\"\n",
    "\n",
    "pipeline = {\n",
    "    \n",
    "    \"ner\" : {Col1 : \"Additional Named Entity Recognition\", Col2 : \"Doc.ents, Doc[i].ent_iob, Doc[i].ent_type\", Col3 : \"Specific tuning for the dataset\"},\n",
    "    \"ncr\" : {Col1 : \"Named Concept Recognition\", Col2 : \"Doc._.named_concepts, span._.concept, span._.attribute, span._.feature, span._.classification\", Col3 : \"Named Concepts from the group schema.\"}\n",
    "    \n",
    "}\n",
    "\n",
    "display(pd.DataFrame(pipeline).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The spaCy language model\n",
    "\n",
    "The spaCy module is based on langugage models which provide a reference dataset for predicting linguistic annotations. spaCy v2.0 features neural models for tagging, parsing and entity recognition. \n",
    "\n",
    "For english there are 8 models listed as follows\n",
    "1. en_core_web_sm: English multi-task CNN trained on OntoNotes. Assigns context-specific token vectors, POS tags, dependency parse and named entities.\n",
    "\n",
    "2. en_core_web_md: English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities\n",
    "\n",
    "3. en_core_web_lg: English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities.\n",
    "\n",
    "4. en_vectors_web_lg\n",
    "\n",
    "5. en_trf_bertbaseuncased_lg: Provides weights and configuration for the pretrained transformer model bert-base-uncased, published by Google Research. The package uses HuggingFace's transformers implementation of the model. Pretrained transformer models assign detailed contextual word representations, using knowledge drawn from a large corpus of unlabelled text. You can use the contextual word representations as features in a variety of pipeline components that can be trained on your own data.\n",
    "\n",
    "6. en_trf_robertabase_lg: Provides weights and configuration for the pretrained transformer model roberta-base, published by Facebook. The package uses HuggingFace's transformers implementation of the model. Pretrained transformer models assign detailed contextual word representations, using knowledge drawn from a large corpus of unlabelled text. You can use the contextual word representations as features in a variety of pipeline components that can be trained on your own data.\n",
    "\n",
    "7. en_trf_distilbertbaseuncased_lg: Provides weights and configuration for the pretrained transformer model distilbert-base-uncased, published by Hugging Face. The package uses HuggingFace's transformers implementation of the model. Pretrained transformer models assign detailed contextual word representations, using knowledge drawn from a large corpus of unlabelled text. You can use the contextual word representations as features in a variety of pipeline components that can be trained on your own data.\n",
    "\n",
    "8. en_trf_xlnetbasecased_lg: Provides weights and configuration for the pretrained transformer model xlnet-base-cased, published by CMU and Google Brain. The package uses HuggingFace's transformers implementation of the model. Pretrained transformer models assign detailed contextual word representations, using knowledge drawn from a large corpus of unlabelled text. You can use the contextual word representations as features in a variety of pipeline components that can be trained on your own data.\n",
    "\n",
    "The accuracy of an nlp tasks very much depends on the model. \n",
    "\n",
    "This test uses en_core_web_md."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
