{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Analysis of I Have A Dream\n",
    "---\n",
    "In this experiment we use sentiment analysis provided by TextBlob and topic modelling. The following libraries are used:\n",
    "\n",
    "NLTK\n",
    "TextBlob\n",
    "Gensim\n",
    "Sentiment analysis is a technique used to understand the sentiment of a body of text through either scoring of words or classification relative to a reference dataset. TextBlob uses a sentiment analysis algorithm provided by the Computational and Psycholinguistics Research Centre (CLiPS). The method is based on crowdsourcing scores of individual words from which sentiment is derived by averaging word scores in a string. The result is a numerical value between -1.0 to indicate negativity and +1.0 to indicate positivity.\n",
    "\n",
    "Topic Modelling is a statistical method used to reveal latent themes in a text by grouping terms together in a single topic. For example, the terms, 'patient', 'doctor', 'disease', 'cancer', ad 'health' will represents topic 'healthcare'.\n",
    "\n",
    "In this notebook, these techniques are applied to Hitler's manifesto, Mein Kampf, as a way to test each of these technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the four 'I have a Dream' statements\n",
    "\n",
    "Before the main experiment, the \"Four I Have A Dream\" statements are testing for sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment:  0.1621212121212121\n",
      "I have a dream that one day this nation will rise up and live out the true meaning of its creed: We hold these truths to be self-evident, that all men are created equal.\n",
      "-----\n",
      "Sentiment:  0.06888888888888889\n",
      "I have a dream that one day on the red hills of Georgia, the sons of former slaves and the sons of former slave owners will be able to sit down together at the table of brotherhood.\n",
      "-----\n",
      "Sentiment:  0.0\n",
      "I have a dream that one day even the state of Mississippi, a state sweltering with the heat of injustice, sweltering with the heat of oppression, will be transformed into an oasis of freedom and justice.\n",
      "-----\n",
      "Sentiment:  -0.025568181818181823\n",
      "I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character.\n",
      "-----\n",
      "Sentiment:  -0.11215728715728718\n",
      "I have a dream that one day down in Alabama, with its vicious racists, with its governor having his lips dripping with the words of interposition and nullification, that one day right down in Alabama little black boys and black girls will be able to join hands with little white boys and white girls as sisters and brothers.\n",
      "-----\n",
      "Sentiment:  -0.022857142857142854\n",
      "I have a dream that one day every valley shall be exhalted, every hill and mountain shall be made low, the rough places will be made plain, and the crooked places will be made straight, and the glory of the Lord shall be revealed, and all flesh shall see it together.\n",
      "-----\n",
      "Wall time: 4.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from textblob import TextBlob\n",
    "import os\n",
    "\n",
    "filepath  = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CulturalViolence\\KnowledgeBases\\data\"\n",
    "\n",
    "dream_text = [\n",
    "    'I have a dream that one day this nation will rise up and live out the true meaning of its creed: We hold these truths to be self-evident, that all men are created equal.',\n",
    "    'I have a dream that one day on the red hills of Georgia, the sons of former slaves and the sons of former slave owners will be able to sit down together at the table of brotherhood.',\n",
    "    'I have a dream that one day even the state of Mississippi, a state sweltering with the heat of injustice, sweltering with the heat of oppression, will be transformed into an oasis of freedom and justice.',\n",
    "    'I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character.',\n",
    "    'I have a dream that one day down in Alabama, with its vicious racists, with its governor having his lips dripping with the words of interposition and nullification, that one day right down in Alabama little black boys and black girls will be able to join hands with little white boys and white girls as sisters and brothers.',\n",
    "    'I have a dream that one day every valley shall be exhalted, every hill and mountain shall be made low, the rough places will be made plain, and the crooked places will be made straight, and the glory of the Lord shall be revealed, and all flesh shall see it together.'\n",
    "]\n",
    "\n",
    "for dream in dream_text:\n",
    "    i = TextBlob(''.join(dream))\n",
    "    print('Sentiment: ', i.sentiment.polarity)\n",
    "    print(dream)\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Sentiment Against Noun Phrases\n",
    "\n",
    "In this next text we test the following statement for sentiment against each of the noun phrases to which it refers and show how references to 'black' as a race will consistently score negatively to when the term 'white' is used.\n",
    "\n",
    "I have a dream that one day down in Alabama, with its vicious racists, with its governor having his lips dripping with the words of interposition and nullification, that one day right down in Alabama little black boys and black girls will be able to join hands with little white boys and white girls as sisters and brothers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment:  -0.17708333333333331\n",
      "little black boys\n",
      "-----\n",
      "Sentiment:  -0.16666666666666666\n",
      "black girls\n",
      "-----\n",
      "Sentiment:  -0.09375\n",
      "little white boys\n",
      "-----\n",
      "Sentiment:  0.0\n",
      "white girls\n",
      "-----\n",
      "Sentiment:  -0.16666666666666666\n",
      "black\n",
      "-----\n",
      "Sentiment:  0.0\n",
      "white\n",
      "-----\n",
      "Sentiment:  -0.1875\n",
      "little\n",
      "-----\n",
      "Wall time: 20.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "dream_text = ['little black boys',\n",
    "              'black girls',\n",
    "              'little white boys',\n",
    "              'white girls',\n",
    "              'black',\n",
    "              'white',\n",
    "              'little'\n",
    "    ]\n",
    "\n",
    "for dream in dream_text:\n",
    "    i = TextBlob(''.join(dream))\n",
    "    print('Sentiment: ', i.sentiment.polarity)\n",
    "    print(dream)\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Main Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Text Topics: [(0, '0.023*\"freedom\" + 0.017*\"must\" + 0.013*\"justice\"'), (1, '0.019*\"negro\" + 0.019*\"freedom\" + 0.019*\"back\"'), (2, '0.023*\"dream\" + 0.015*\"come\" + 0.015*\"today\"')]\n",
      "-----\n",
      "Text Overall Sentiment: 0.14083845280536458\n",
      "-----\n",
      "Text Common Nouns: [('negro', 15), ('freedom ring', 11), ('america', 5), ('mississippi', 4), ('god', 3), (\"'s children\", 3), ('alabama', 3), ('georgia', 3), ('black men', 2), ('white men', 2), ('insufficient funds', 2), ('police brutality', 2), ('york', 2), ('free', 2), ('score years', 1)]\n",
      "-----\n",
      "Speech common words: [('freedom', 20), ('negro', 15), ('one', 13), ('let', 13), ('ring', 12)]\n",
      "-----\n",
      "With a score of  0.8 , the most positive paragraph less than 1 is:\n",
      "But we refuse to believe that the bank of justice is bankrupt. We refuse to believe that there are insufficient funds in the great vaults of opportunity of this nation. So we have come to cash this check- a check that will give us upon demand the riches of freedom and the security of justice.\n",
      "\n",
      "-----\n",
      "With a score of  -0.14999999999999997 , the most negative paragraph greater than -1 is:\n",
      "It is obvious today that America has defaulted on this promissory note insofar as her citizens of color are concerned. Instead of honoring this sacred obligation, America has given the Negro people a bad check; a check which has come back marked \"insufficient funds.\"\n",
      "\n",
      "-----\n",
      "The paragraphs with a sentiment score of 1 are:\n",
      "\n",
      "The paragraphs with a sentiment score of -1 are:\n",
      "-----\n",
      "Paragraph 1\n",
      "    Paragraph topics: [(0, '0.111*\"freedom\" + 0.111*\"go\"'), (1, '0.111*\"go\" + 0.111*\"greatest\"'), (2, '0.180*\"history\" + 0.103*\"demonstration\"')]\n",
      "    Paragraph sentiment: 0.5481481481481482\n",
      "    Paragraph nouns: []\n",
      "    Paragraph common words: [('history', 2), ('happy', 1), ('join', 1), ('today', 1), ('go', 1)]\n",
      "\n",
      "Paragraph 2\n",
      "    Paragraph topics: [(0, '0.030*\"american\" + 0.030*\"five\"'), (1, '0.051*\"came\" + 0.051*\"great\"'), (2, '0.030*\"joyous\" + 0.030*\"signed\"')]\n",
      "    Paragraph sentiment: 0.39\n",
      "    Paragraph nouns: ['score years', 'symbolic shadow', 'emancipation proclamation', 'momentous decree', 'great beacon', 'negro', 'joyous daybreak', 'long night']\n",
      "    Paragraph common words: [('great', 2), ('came', 2), ('five', 1), ('score', 1), ('year', 1)]\n",
      "\n",
      "Paragraph 3\n",
      "    Paragraph topics: [(0, '0.068*\"later\" + 0.068*\"year\"'), (1, '0.029*\"corner\" + 0.029*\"exile\"'), (2, '0.029*\"material\" + 0.029*\"come\"')]\n",
      "    Paragraph sentiment: -0.019999999999999997\n",
      "    Paragraph nouns: ['negro', 'negro', 'negro', 'vast ocean', 'material prosperity', 'negro', 'american society', 'own land', 'shameful condition']\n",
      "    Paragraph common words: [('one', 4), ('hundred', 4), ('year', 4), ('later', 4), ('negro', 4)]\n",
      "\n",
      "Paragraph 4\n",
      "    Paragraph topics: [(0, '0.045*\"weve\" + 0.045*\"nation\"'), (1, '0.045*\"american\" + 0.045*\"cash\"'), (2, '0.045*\"check\" + 0.045*\"heir\"')]\n",
      "    Paragraph sentiment: 0.5\n",
      "    Paragraph nouns: [\"nation 's\", 'capital', 'magnificent words', 'declaration', 'independence', 'promissory note', 'fall heir']\n",
      "    Paragraph common words: [('sense', 1), ('weve', 1), ('come', 1), ('nation', 1), ('capital', 1)]\n",
      "\n",
      "Paragraph 5\n",
      "    Paragraph topics: [(0, '0.067*\"happiness\" + 0.067*\"life\"'), (1, '0.067*\"black\" + 0.067*\"guaranteed\"'), (2, '0.152*\"men\" + 0.061*\"would\"')]\n",
      "    Paragraph sentiment: 0.17777777777777778\n",
      "    Paragraph nouns: ['black men', 'white men', 'unalienable rights']\n",
      "    Paragraph common words: [('men', 3), ('note', 1), ('promise', 1), ('yes', 1), ('black', 1)]\n",
      "\n",
      "Paragraph 6\n",
      "    Paragraph topics: [(0, '0.042*\"back\" + 0.042*\"concerned\"'), (1, '0.069*\"america\" + 0.069*\"check\"'), (2, '0.042*\"citizen\" + 0.042*\"today\"')]\n",
      "    Paragraph sentiment: -0.14999999999999997\n",
      "    Paragraph nouns: ['america', 'promissory note insofar', 'america', 'negro', 'bad check', 'insufficient funds']\n",
      "    Paragraph common words: [('america', 2), ('check', 2), ('obvious', 1), ('today', 1), ('defaulted', 1)]\n",
      "\n",
      "Paragraph 7\n",
      "    Paragraph topics: [(0, '0.048*\"opportunity\" + 0.048*\"freedom\"'), (1, '0.048*\"cash\" + 0.048*\"opportunity\"'), (2, '0.073*\"check\" + 0.073*\"justice\"')]\n",
      "    Paragraph sentiment: 0.8\n",
      "    Paragraph nouns: ['insufficient funds', 'great vaults']\n",
      "    Paragraph common words: [('refuse', 2), ('believe', 2), ('justice', 2), ('check', 2), ('bank', 1)]\n",
      "\n",
      "Paragraph 8\n",
      "    Paragraph topics: [(0, '0.059*\"gradualism\" + 0.059*\"hallowed\"'), (1, '0.059*\"drug\" + 0.059*\"urgency\"'), (2, '0.059*\"also\" + 0.059*\"america\"')]\n",
      "    Paragraph sentiment: 0.0\n",
      "    Paragraph nouns: ['america', 'fierce urgency']\n",
      "    Paragraph common words: [('also', 1), ('come', 1), ('hallowed', 1), ('spot', 1), ('remind', 1)]\n",
      "\n",
      "Paragraph 9\n",
      "    Paragraph topics: [(0, '0.042*\"injustice\" + 0.042*\"nation\"'), (1, '0.114*\"time\" + 0.061*\"justice\"'), (2, '0.042*\"child\" + 0.042*\"desolate\"')]\n",
      "    Paragraph sentiment: 0.016666666666666673\n",
      "    Paragraph nouns: ['desolate valley', 'sunlit path', 'racial justice', 'racial injustice', 'solid rock', 'god', \"'s children\"]\n",
      "    Paragraph common words: [('time', 4), ('make', 2), ('racial', 2), ('justice', 2), ('real', 1)]\n",
      "\n",
      "Paragraph 10\n",
      "    Paragraph topics: [(0, '0.022*\"whirlwind\" + 0.022*\"content\"'), (1, '0.022*\"citizenship\" + 0.022*\"discontent\"'), (2, '0.051*\"nation\" + 0.051*\"negro\"')]\n",
      "    Paragraph sentiment: 0.05000000000000001\n",
      "    Paragraph nouns: ['negro', 'legitimate discontent', 'nineteen', 'negro', 'nation returns', 'america', 'negro', 'citizenship rights', 'bright day']\n",
      "    Paragraph common words: [('nation', 3), ('negro', 3), ('would', 1), ('fatal', 1), ('overlook', 1)]\n",
      "\n",
      "Paragraph 11\n",
      "    Paragraph topics: [(0, '0.079*\"must\" + 0.035*\"force\"'), (1, '0.022*\"gaining\" + 0.022*\"creative\"'), (2, '0.022*\"people\" + 0.022*\"forever\"')]\n",
      "    Paragraph sentiment: 0.12666666666666668\n",
      "    Paragraph nouns: ['warm threshold', 'rightful place', 'wrongful deeds', 'high plane', 'physical violence', 'majestic heights', 'physical force', 'soul force']\n",
      "    Paragraph common words: [('must', 5), ('physical', 2), ('force', 2), ('something', 1), ('say', 1)]\n",
      "\n",
      "Paragraph 12\n",
      "    Paragraph topics: [(0, '0.037*\"engulfed\" + 0.037*\"marvelous\"'), (1, '0.057*\"realize\" + 0.057*\"come\"'), (2, '0.037*\"alone\" + 0.037*\"bound\"')]\n",
      "    Paragraph sentiment: 0.32727272727272727\n",
      "    Paragraph nouns: ['new militancy', 'negro', 'white people', 'white brothers']\n",
      "    Paragraph common words: [('white', 2), ('come', 2), ('realize', 2), ('destiny', 2), ('freedom', 2)]\n",
      "\n",
      "Paragraph 13\n",
      "    Paragraph topics: [(0, '0.063*\"satisfied\" + 0.063*\"cannot\"'), (1, '0.063*\"ahead\" + 0.063*\"when\"'), (2, '0.063*\"civil\" + 0.063*\"devotee\"')]\n",
      "    Paragraph sentiment: 0.25\n",
      "    Paragraph nouns: ['civil rights']\n",
      "    Paragraph common words: [('walk', 1), ('must', 1), ('make', 1), ('pledge', 1), ('shall', 1)]\n",
      "\n",
      "Paragraph 14\n",
      "    Paragraph topics: [(0, '0.111*\"brutality\" + 0.111*\"horror\"'), (1, '0.111*\"brutality\" + 0.111*\"horror\"'), (2, '0.111*\"brutality\" + 0.111*\"horror\"')]\n",
      "    Paragraph sentiment: 0.125\n",
      "    Paragraph nouns: ['negro', 'unspeakable horrors', 'police brutality']\n",
      "    Paragraph common words: [('never', 1), ('satisfied', 1), ('long', 1), ('negro', 1), ('victim', 1)]\n",
      "\n",
      "Paragraph 15\n",
      "    Paragraph topics: [(0, '0.071*\"city\" + 0.071*\"highway\"'), (1, '0.071*\"gain\" + 0.071*\"body\"'), (2, '0.071*\"body\" + 0.071*\"cannot\"')]\n",
      "    Paragraph sentiment: 0.08333333333333333\n",
      "    Paragraph nouns: []\n",
      "    Paragraph common words: [('never', 1), ('satisfied', 1), ('long', 1), ('body', 1), ('heavy', 1)]\n",
      "\n",
      "Paragraph 16\n",
      "    Paragraph topics: [(0, '0.100*\"long\" + 0.100*\"basic\"'), (1, '0.100*\"cannot\" + 0.100*\"negro\"'), (2, '0.100*\"basic\" + 0.100*\"cannot\"')]\n",
      "    Paragraph sentiment: 0.09\n",
      "    Paragraph nouns: ['negro', 'basic mobility']\n",
      "    Paragraph common words: [('cannot', 1), ('satisfied', 1), ('long', 1), ('negro', 1), ('basic', 1)]\n",
      "\n",
      "Paragraph 17\n",
      "    Paragraph topics: [(0, '0.077*\"chlidren\" + 0.077*\"dignity\"'), (1, '0.077*\"sign\" + 0.077*\"never\"'), (2, '0.077*\"chlidren\" + 0.077*\"dignity\"')]\n",
      "    Paragraph sentiment: 0.15\n",
      "    Paragraph nouns: []\n",
      "    Paragraph common words: [('never', 1), ('satisfied', 1), ('long', 1), ('chlidren', 1), ('stripped', 1)]\n",
      "\n",
      "Paragraph 18\n",
      "    Paragraph topics: [(0, '0.100*\"belief\" + 0.100*\"long\"'), (1, '0.143*\"cannot\" + 0.143*\"negro\"'), (2, '0.100*\"belief\" + 0.100*\"long\"')]\n",
      "    Paragraph sentiment: 0.19545454545454546\n",
      "    Paragraph nouns: ['negro', 'mississippi', 'negro', 'york']\n",
      "    Paragraph common words: [('cannot', 2), ('negro', 2), ('vote', 2), ('satisfied', 1), ('long', 1)]\n",
      "\n",
      "Paragraph 19\n",
      "    Paragraph topics: [(0, '0.156*\"satisfied\" + 0.156*\"no\"'), (1, '0.111*\"roll\" + 0.111*\"justice\"'), (2, '0.111*\"justice\" + 0.111*\"mighty\"')]\n",
      "    Paragraph sentiment: 0.12361111111111112\n",
      "    Paragraph nouns: ['justice rolls']\n",
      "    Paragraph common words: [('no', 2), ('satisfied', 2), ('like', 2), ('justice', 1), ('roll', 1)]\n",
      "\n",
      "Paragraph 20\n",
      "    Paragraph topics: [(0, '0.083*\"come\" + 0.058*\"suffering\"'), (1, '0.036*\"creative\" + 0.036*\"continue\"'), (2, '0.036*\"jail\" + 0.036*\"cell\"')]\n",
      "    Paragraph sentiment: 0.2166666666666667\n",
      "    Paragraph nouns: ['great trials', 'narrow jail cells', 'police brutality', 'continue']\n",
      "    Paragraph common words: [('come', 3), ('suffering', 2), ('unmindful', 1), ('great', 1), ('trial', 1)]\n",
      "\n",
      "Paragraph 21\n",
      "    Paragraph topics: [(0, '0.048*\"city\" + 0.048*\"ghetto\"'), (1, '0.167*\"go\" + 0.167*\"back\"'), (2, '0.048*\"city\" + 0.048*\"ghetto\"')]\n",
      "    Paragraph sentiment: 0.0\n",
      "    Paragraph nouns: ['mississippi', 'alabama', 'carolina', 'georgia', 'louisiana', 'northern cities']\n",
      "    Paragraph common words: [('go', 6), ('back', 6), ('mississippi', 1), ('alabama', 1), ('south', 1)]\n",
      "\n",
      "Paragraph 22\n",
      "    Paragraph topics: [(0, '0.077*\"though\" + 0.077*\"still\"'), (1, '0.164*\"dream\" + 0.115*\"today\"'), (2, '0.077*\"american\" + 0.077*\"deeply\"')]\n",
      "    Paragraph sentiment: 0.0\n",
      "    Paragraph nouns: ['american dream']\n",
      "    Paragraph common words: [('dream', 3), ('today', 2), ('say', 1), ('friend', 1), ('even', 1)]\n",
      "\n",
      "Paragraph 23\n",
      "    Paragraph topics: [(0, '0.063*\"day\" + 0.063*\"hold\"'), (1, '0.063*\"day\" + 0.063*\"hold\"'), (2, '0.062*\"created\" + 0.062*\"creed\"')]\n",
      "    Paragraph sentiment: 0.1621212121212121\n",
      "    Paragraph nouns: ['true meaning']\n",
      "    Paragraph common words: [('dream', 1), ('one', 1), ('day', 1), ('nation', 1), ('rise', 1)]\n",
      "\n",
      "Paragraph 24\n",
      "    Paragraph topics: [(0, '0.067*\"able\" + 0.067*\"brotherhood\"'), (1, '0.101*\"former\" + 0.101*\"slave\"'), (2, '0.067*\"sit\" + 0.067*\"brotherhood\"')]\n",
      "    Paragraph sentiment: 0.06888888888888889\n",
      "    Paragraph nouns: ['red hills', 'georgia', 'slave owners']\n",
      "    Paragraph common words: [('son', 2), ('former', 2), ('slave', 2), ('dream', 1), ('one', 1)]\n",
      "\n",
      "Paragraph 25\n",
      "    Paragraph topics: [(0, '0.071*\"day\" + 0.071*\"freedom\"'), (1, '0.108*\"heat\" + 0.108*\"state\"'), (2, '0.071*\"oppression\" + 0.071*\"even\"')]\n",
      "    Paragraph sentiment: 0.0\n",
      "    Paragraph nouns: ['mississippi']\n",
      "    Paragraph common words: [('state', 2), ('sweltering', 2), ('heat', 2), ('dream', 1), ('one', 1)]\n",
      "\n",
      "Paragraph 26\n",
      "    Paragraph topics: [(0, '0.077*\"color\" + 0.077*\"character\"'), (1, '0.077*\"dream\" + 0.077*\"color\"'), (2, '0.077*\"little\" + 0.077*\"child\"')]\n",
      "    Paragraph sentiment: -0.025568181818181823\n",
      "    Paragraph nouns: []\n",
      "    Paragraph common words: [('dream', 1), ('four', 1), ('little', 1), ('child', 1), ('one', 1)]\n",
      "\n",
      "Paragraph 27\n",
      "    Paragraph topics: [(0, '0.500*\"dream\" + 0.500*\"today\"'), (1, '0.500*\"dream\" + 0.500*\"today\"'), (2, '0.500*\"today\" + 0.500*\"dream\"')]\n",
      "    Paragraph sentiment: 0.0\n",
      "    Paragraph nouns: []\n",
      "    Paragraph common words: [('dream', 1), ('today', 1)]\n",
      "\n",
      "Paragraph 28\n",
      "    Paragraph topics: [(0, '0.043*\"dripping\" + 0.043*\"interposition\"'), (1, '0.060*\"little\" + 0.060*\"one\"'), (2, '0.043*\"word\" + 0.043*\"interposition\"')]\n",
      "    Paragraph sentiment: -0.11215728715728718\n",
      "    Paragraph nouns: ['alabama', 'vicious racists', 'alabama', 'black boys', 'black girls', 'white boys', 'white girls']\n",
      "    Paragraph common words: [('one', 2), ('day', 2), ('alabama', 2), ('little', 2), ('black', 2)]\n",
      "\n",
      "Paragraph 29\n",
      "    Paragraph topics: [(0, '0.500*\"today\" + 0.500*\"dream\"'), (1, '0.500*\"dream\" + 0.500*\"today\"'), (2, '0.500*\"today\" + 0.500*\"dream\"')]\n",
      "    Paragraph sentiment: 0.0\n",
      "    Paragraph nouns: []\n",
      "    Paragraph common words: [('dream', 1), ('today', 1)]\n",
      "\n",
      "Paragraph 30\n",
      "    Paragraph topics: [(0, '0.045*\"exhalted\" + 0.045*\"crooked\"'), (1, '0.045*\"crooked\" + 0.045*\"day\"'), (2, '0.119*\"shall\" + 0.092*\"made\"')]\n",
      "    Paragraph sentiment: -0.022857142857142854\n",
      "    Paragraph nouns: ['rough places']\n",
      "    Paragraph common words: [('shall', 4), ('made', 3), ('every', 2), ('place', 2), ('dream', 1)]\n",
      "\n",
      "Paragraph 31\n",
      "    Paragraph topics: [(0, '0.056*\"stone\" + 0.056*\"nation\"'), (1, '0.119*\"faith\" + 0.083*\"able\"'), (2, '0.056*\"jangling\" + 0.056*\"go\"')]\n",
      "    Paragraph sentiment: 0.4625\n",
      "    Paragraph nouns: ['beautiful symphony']\n",
      "    Paragraph common words: [('faith', 3), ('hope', 2), ('able', 2), ('go', 1), ('back', 1)]\n",
      "\n",
      "Paragraph 32\n",
      "    Paragraph topics: [(0, '0.071*\"able\" + 0.071*\"free\"'), (1, '0.071*\"able\" + 0.071*\"day\"'), (2, '0.235*\"together\" + 0.059*\"day\"')]\n",
      "    Paragraph sentiment: 0.26666666666666666\n",
      "    Paragraph nouns: []\n",
      "    Paragraph common words: [('together', 5), ('faith', 1), ('able', 1), ('work', 1), ('pray', 1)]\n",
      "\n",
      "Paragraph 33\n",
      "    Paragraph topics: [(0, '0.043*\"died\" + 0.043*\"freedom\"'), (1, '0.043*\"country\" + 0.043*\"my\"'), (2, '0.096*\"land\" + 0.067*\"sing\"')]\n",
      "    Paragraph sentiment: 0.3287878787878788\n",
      "    Paragraph nouns: ['god', \"'s children\", 'new meaning', \"country 't\", 'sweet land', 'land', 'pilgrims', 'freedom ring']\n",
      "    Paragraph common words: [('land', 3), ('sing', 2), ('thee', 2), ('day', 1), ('god', 1)]\n",
      "\n",
      "Paragraph 34\n",
      "    Paragraph topics: [(0, '0.103*\"freedom\" + 0.103*\"let\"'), (1, '0.053*\"hilltop\" + 0.053*\"america\"'), (2, '0.053*\"allegheny\" + 0.053*\"america\"')]\n",
      "    Paragraph sentiment: 0.36454545454545445\n",
      "    Paragraph nouns: ['america', 'great nation', 'freedom ring', 'prodigious hilltops', 'hampshire', 'freedom ring', 'york', 'freedom ring', 'alleghenies', 'pennsylvania']\n",
      "    Paragraph common words: [('let', 3), ('freedom', 3), ('ring', 3), ('new', 2), ('america', 1)]\n",
      "\n",
      "Paragraph 35\n",
      "    Paragraph topics: [(0, '0.144*\"let\" + 0.144*\"freedom\"'), (1, '0.067*\"california\" + 0.067*\"colorado\"'), (2, '0.067*\"lookout\" + 0.067*\"georgia\"')]\n",
      "    Paragraph sentiment: 0.0\n",
      "    Paragraph nouns: ['freedom ring', 'rockies', 'colorado', 'freedom ring', 'curvaceous slopes', 'california', 'freedom ring', 'stone mountain', 'georgia', 'freedom ring', 'lookout mountain', 'tennessee']\n",
      "    Paragraph common words: [('let', 4), ('freedom', 4), ('ring', 4), ('mountain', 2), ('snowcapped', 1)]\n",
      "\n",
      "Paragraph 36\n",
      "    Paragraph topics: [(0, '0.159*\"ring\" + 0.159*\"freedom\"'), (1, '0.125*\"mountainside\" + 0.125*\"molehill\"'), (2, '0.125*\"hill\" + 0.125*\"mississippi\"')]\n",
      "    Paragraph sentiment: 0.0\n",
      "    Paragraph nouns: ['freedom ring', 'mississippi', 'freedom ring']\n",
      "    Paragraph common words: [('let', 2), ('freedom', 2), ('ring', 2), ('every', 2), ('hill', 1)]\n",
      "\n",
      "Paragraph 37\n",
      "    Paragraph topics: [(0, '0.079*\"every\" + 0.061*\"free\"'), (1, '0.030*\"join\" + 0.030*\"negro\"'), (2, '0.030*\"join\" + 0.030*\"spiritual\"')]\n",
      "    Paragraph sentiment: 0.17777777777777778\n",
      "    Paragraph nouns: ['freedom ring', 'god', \"'s children\", 'black men', 'white men', 'jews', 'protestants', 'negro', 'free', 'free', 'thank god almighty']\n",
      "    Paragraph common words: [('every', 4), ('free', 3), ('last', 3), ('ring', 2), ('able', 2)]\n",
      "\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### analyse the whole document to extract the topics and sentiment\n",
    "# https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "# https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n",
    "\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import gensim\n",
    "import collections\n",
    "from gensim import corpora\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "### open file\n",
    "filepath  = r\"C:\\Users\\Steve\\OneDrive - University of Southampton\\CulturalViolence\\KnowledgeBases\\data\"\n",
    "\n",
    "with open(os.path.join(filepath, \"IHaveADream.txt\"), 'r') as text:\n",
    "    speech_text = text.readlines()\n",
    "\n",
    "# define common functions\n",
    "def clean(doc): # text cleaning function\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop]) # remove stopwords\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude) # remove punctuation\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split()) #lemmatize words\n",
    "    return normalized\n",
    "\n",
    "def Most_Common(lst, quantity):\n",
    "    data = Counter(lst)\n",
    "    return data.most_common(quantity)\n",
    "\n",
    "# speech pre-processing to a bag-of-words\n",
    "i = TextBlob(''.join(speech_text))\n",
    "doc_clean = [clean(doc).split() for doc in speech_text] # clean speech to a bag of words\n",
    "dictionary = corpora.Dictionary(doc_clean) # Creating the term dictionary for the speech, where every unique term is assigned an index.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean] # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "print('Overall Text Topics: ', end='')\n",
    "print(ldamodel.print_topics(num_topics=10, num_words=3))\n",
    "print('-----')\n",
    "\n",
    "print('Text Overall Sentiment: ', end='')\n",
    "print(i.sentiment.polarity)\n",
    "print('-----')\n",
    "\n",
    "print('Text Common Nouns: ', end='')\n",
    "print(Most_Common(i.noun_phrases, 15))\n",
    "print('-----')\n",
    "\n",
    "flat_list = []\n",
    "\n",
    "for sublist in doc_clean:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "\n",
    "\n",
    "print('Speech common words: ', end='')\n",
    "print(Most_Common(flat_list, 5))\n",
    "print('-----')\n",
    "\n",
    "# generate speech summary data\n",
    "\n",
    "counter = 0\n",
    "most_pos_para = ''\n",
    "most_neg_para = ''\n",
    "pos_paras = []\n",
    "neg_paras = []\n",
    "most_pos_score = 0\n",
    "most_neg_score = 0\n",
    "\n",
    "for section in speech_text:\n",
    "    paragraph = TextBlob(section)\n",
    "    \n",
    "    if paragraph.sentiment.polarity > most_pos_score and paragraph.sentiment.polarity < 1:\n",
    "        most_pos_para = paragraph\n",
    "        most_pos_score = paragraph.sentiment.polarity\n",
    "    elif paragraph.sentiment.polarity < most_neg_score and paragraph.sentiment.polarity > -1:\n",
    "        most_neg_para = paragraph\n",
    "        most_neg_score = paragraph.sentiment.polarity\n",
    "    elif paragraph.sentiment.polarity == 1:\n",
    "        pos_paras.append(section)\n",
    "    elif paragraph.sentiment.polarity == -1:\n",
    "        neg_paras.append(section)\n",
    "    \n",
    "        \n",
    "print('With a score of ', str(most_pos_score), ', the most positive paragraph less than 1 is:')\n",
    "print(most_pos_para)\n",
    "print('-----')\n",
    "print('With a score of ', str(most_neg_score), ', the most negative paragraph greater than -1 is:')\n",
    "print(most_neg_para)\n",
    "print('-----')\n",
    "print('The paragraphs with a sentiment score of 1 are:')\n",
    "for i in pos_paras:\n",
    "    print(i)\n",
    "print()\n",
    "print('The paragraphs with a sentiment score of -1 are:')\n",
    "for i in neg_paras:\n",
    "    print(i)\n",
    "print('-----')\n",
    "\n",
    "# generate paragraph summary data by iterating through each paragraph\n",
    "\n",
    "for section in speech_text:\n",
    "    counter += 1\n",
    "    paragraph = TextBlob(section)\n",
    "    \n",
    "    doc_clean = [clean(section).split()] # clean section\n",
    "    dictionary = corpora.Dictionary(doc_clean) # Creating the term dictionary of each paragraph, where every unique term is assigned an index. \n",
    "    \n",
    "    if len(dictionary) > 0: ## skip over empty paragraphs\n",
    "        doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean] # Converting paragraph into Document Term Matrix using dictionary prepared above.\n",
    "        ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "    \n",
    "    print('Paragraph %d' %counter)\n",
    "    print('    Paragraph topics: ', end='')\n",
    "    print(ldamodel.print_topics(num_topics=5, num_words=2))\n",
    "    print('    Paragraph sentiment: ', end='')\n",
    "    print(paragraph.sentiment.polarity)\n",
    "    print('    Paragraph nouns: ', end='')\n",
    "    print(paragraph.noun_phrases)\n",
    "    print('    Paragraph common words: ', end='')\n",
    "    \n",
    "    for w in doc_clean:\n",
    "        print(Most_Common(w, 5))\n",
    "        \n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
